{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/longphi1080/adversarial-attacks-defences-on-fruit-images?scriptVersionId=91120770\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Preparation","metadata":{}},{"cell_type":"code","source":"from __future__ import print_function\nimport argparse\nimport os\nimport random\n\nimport numpy as np\nimport numpy.matlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\n%matplotlib inline\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.autograd import Variable\n\nimport torchvision\nimport torchvision.datasets as dset\nimport torchvision.transforms.functional as TF\nimport torchvision.utils as vutils\nfrom torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n\nfrom scipy.spatial.distance import cdist","metadata":{"papermill":{"duration":1.868959,"end_time":"2020-08-28T14:03:27.558422","exception":false,"start_time":"2020-08-28T14:03:25.689463","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-23T15:44:14.766882Z","iopub.execute_input":"2022-03-23T15:44:14.767154Z","iopub.status.idle":"2022-03-23T15:44:14.780604Z","shell.execute_reply.started":"2022-03-23T15:44:14.767122Z","shell.execute_reply":"2022-03-23T15:44:14.779991Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"font = FontProperties()\nfont.set_family('serif')","metadata":{"execution":{"iopub.status.busy":"2022-03-23T15:56:16.149401Z","iopub.execute_input":"2022-03-23T15:56:16.15014Z","iopub.status.idle":"2022-03-23T15:56:16.153488Z","shell.execute_reply.started":"2022-03-23T15:56:16.150107Z","shell.execute_reply":"2022-03-23T15:56:16.15288Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Let's look at the classes from directories\ndata_dir = 'C:\\\\Users\\\\nguye\\\\OneDrive\\\\Documents\\\\Fruits'\nclasses = os.listdir(data_dir)\nprint(\"There are {} fruits.\".format(len(classes)))","metadata":{"execution":{"iopub.execute_input":"2020-08-28T14:05:35.965027Z","iopub.status.busy":"2020-08-28T14:05:35.963203Z","iopub.status.idle":"2020-08-28T14:05:35.967817Z","shell.execute_reply":"2020-08-28T14:05:35.966986Z"},"papermill":{"duration":0.017335,"end_time":"2020-08-28T14:05:35.96794","exception":false,"start_time":"2020-08-28T14:05:35.950605","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transform images","metadata":{}},{"cell_type":"markdown","source":"## Resize images","metadata":{}},{"cell_type":"code","source":"# Max = 258\npixels = 64\n\ncomposed = Compose([Resize((pixels, pixels)), ToTensor()])\n\ndataset = torchvision.datasets.ImageFolder(data_dir, composed)\nprint(\"There are {} images.\".format(len(dataset)))\n\n# An example image\nplt.imshow(dataset[6][0].permute(1,2,0))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bit-depth reduction","metadata":{}},{"cell_type":"code","source":"# Reduce bit-depth\npixels = 258\nbits = 20\n\nclass BitDepthReduction:\n    def __call__(self, image):\n        image = TF.pil_to_tensor(image).float() / 255\n        return torch.round(image * (2^bits - 1))/(2^bits - 1)\n\ncomposed = Compose([Resize((pixels, pixels)), BitDepthReduction()])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = torchvision.datasets.ImageFolder(data_dir, composed)\nprint(\"There are {} images.\".format(len(dataset)))\n\n# An example image\nplt.imshow(dataset[6][0].permute(1,2,0))\nplt.savefig('figures/8-bits-apples.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Resize to [pixel * x, pixel) & pad to pixel","metadata":{}},{"cell_type":"code","source":"# Define image size & minimum resize size\npixels = 264\nmin_resize_ratio = 0.95","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"minimum_resize = np.round_(pixels * min_resize_ratio)\nprint(\"Minimum resized size of image is:\", minimum_resize, \"pixels.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RandomResize:\n    def __call__(self, image):\n        new_w = random.randint(minimum_resize, pixels)\n        new_h = random.randint(minimum_resize, pixels)\n        return TF.resize(image, [new_w, new_h])\n\nclass SquarePad:\n    def __call__(self, image):\n        W, H = image.size\n        left = random.randint(0, pixels - W + 1)\n        top = random.randint(0, pixels - W + 1)\n        right = pixels - W - left\n        bottom = pixels - H - top\n        padding = (left, top, right, bottom)\n        return TF.pad(image, padding)\n\ncomposed = Compose([RandomResize(), SquarePad(), ToTensor()])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = torchvision.datasets.ImageFolder(data_dir, composed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pad to pixel only","metadata":{}},{"cell_type":"code","source":"class SquarePad:\n    def __call__(self, image):\n        padding = (3, 3, 3, 3)\n        return TF.pad(image, padding)\n\ncomposed = Compose([Resize((258, 258)), SquarePad(), ToTensor()])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = torchvision.datasets.ImageFolder(data_dir, composed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## k-mean bit-depth reduction","metadata":{}},{"cell_type":"code","source":"class k_mean_bit_depth:\n    def __call__(self, image):\n        centroids = np.random.normal(0, 1, size=(2**8, 3))\n        for N in range(100):\n            if N > 0:\n                L_old = L_new\n            # The assignments:\n            assignments = np.zeros((258**2,2**8))\n            \n            image = np.array(image).reshape(258**2,3)\n            \n            # Distance to centroids\n            dist = cdist(image, centroids)\n\n            # Assignments:\n            boolean = np.argmin(dist, axis = 1)\n            for i in range(len(boolean)):\n                assignments[i, boolean[i]] = 1\n\n            # Objective function:\n            L_new = np.sum(assignments*dist**2)\n\n            # Recalculate centroids:\n            for i in np.unique(boolean):\n                centroids[i,:] = np.mean(image[[j == i for j in boolean]], axis = 0)\n\n            if N > 0:\n                if (L_old - L_new)/L_new <= 1e-9:\n                    return np.matmul(assignments, centroids).reshape(258,258,3)\n        return np.matmul(assignments, centroids).reshape(258,258,3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"composed = Compose([Resize((258, 258)), k_mean_bit_depth(), ToTensor()])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = torchvision.datasets.ImageFolder(data_dir, composed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Divide data into sets","metadata":{}},{"cell_type":"code","source":"# Create training, validation and test sets\nval_size = 10509\ntest_size = 7000\ntrain_size = len(dataset) - val_size - test_size\n\ntrain_ds, val_ds, test_ds = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\nprint(\"Training set size:\", len(train_ds))\nprint(\"Validation set size:\", len(val_ds))\nprint(\"Test set size:\", len(test_ds))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set batch size & create loaders\nbatch_size = 10\ntrain_loader = DataLoader(train_ds, batch_size, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size*2, pin_memory=True)\ntest_loader = DataLoader(test_ds, batch_size*2, pin_memory=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataiter = iter(test_loader)\nimages, labels = next(dataiter)\n# Show an image\nplt.axis('off')\nplt.imshow(images[0].data.detach().cpu().numpy())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define functions","metadata":{"papermill":{"duration":0.019262,"end_time":"2020-08-28T14:07:53.475947","exception":false,"start_time":"2020-08-28T14:07:53.456685","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Evaluate accuracy\ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n\n# Create a base model class (no architecture)\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)\n        loss = F.cross_entropy(out, labels) \n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    \n        loss = F.cross_entropy(out, labels)   \n        acc = accuracy(out, labels)           \n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   \n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      \n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n\n# Training and assessment functions\ndef evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history\n\n# Create a function to port stuff to GPU\ndef get_default_device():\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n\ndevice = get_default_device()\n\n# create a function for transferring data to the GPU\ndef to_device(data, device):\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        return len(self.dl)\n    \n# functions for displaying the error and accuracy of the model\ndef plot_losses(history):\n    losses = [x['val_loss'] for x in history]\n    plt.plot(losses, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.title('Loss vs. No. of epochs')\n\ndef plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs')\n    \n# FGSM attack\ndef fgsm_attack(image, epsilon, data_grad):\n    # Collect the element-wise sign of the data gradient\n    sign_data_grad = data_grad.sign()\n    # Create the perturbed image by adjusting each pixel of the input image\n    perturbed_image = image + epsilon*sign_data_grad\n    # Adding clipping to maintain [0,1] range\n    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n    # Return the perturbed image\n    return perturbed_image","metadata":{"execution":{"iopub.execute_input":"2020-08-28T14:07:53.523004Z","iopub.status.busy":"2020-08-28T14:07:53.521972Z","iopub.status.idle":"2020-08-28T14:07:53.527393Z","shell.execute_reply":"2020-08-28T14:07:53.526655Z"},"papermill":{"duration":0.032211,"end_time":"2020-08-28T14:07:53.527549","exception":false,"start_time":"2020-08-28T14:07:53.495338","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test FGSM","metadata":{}},{"cell_type":"code","source":"def fgsm_attack(image, epsilon, data_grad):\n    # Collect the element-wise sign of the data gradient\n    sign_data_grad = data_grad.sign()\n    # Create the perturbed image by adjusting each pixel of the input image\n    perturbed_image = image + epsilon*sign_data_grad\n    # Adding clipping to maintain [0,1] range\n    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n    # Return the perturbed image\n    return sign_data_grad, perturbed_image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(target_model, defense_model, device, test_loader, epsilon):\n    # Loop over all examples in test set\n    for data, target in test_loader:\n        # Send the data and label to the device\n        data, target = data.to(device), target.to(device)\n\n        # Remember gradients\n        data.requires_grad = True\n\n        # Forward pass the data through the model\n        output = model(data)\n        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        \n        # If the initial prediction is wrong, dont bother attacking, just move on\n        if init_pred.item() != target.item():\n            continue\n\n        # Calculate the loss\n        loss = F.nll_loss(output, target)\n\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect datagrad\n        data_grad = data.grad.data\n\n        # Call FGSM Attack\n        sign_data_grad, perturbed_data = fgsm_attack(data, epsilon, data_grad)\n\n        # Re-classify the perturbed image\n        target_output = target_model(perturbed_data)\n        \n        # Check for success\n        target_pred = target_output.max(1, keepdim=True)[1]\n        \n        print(target_pred)\n        \n        break\n        \n    return data, target, perturbed_data, sign_data_grad","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_model = 'new-model.pth'\nmodel.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = DataLoader(test_ds, batch_size=1, num_workers=4, pin_memory=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data, target, perturbed_image, sign_data_grad = test(model, model, device, test_loader, 0.04)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.axis('off')\nplt.imshow(data[0].data.detach().cpu().numpy().transpose(1,2,0))\nplt.savefig(\"figures/original-kiwi.png\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.axis('off')\nplt.imshow(sign_data_grad[0].data.detach().cpu().numpy().transpose(1,2,0))\nplt.savefig(\"figures/noise.png\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.axis('off')\nplt.imshow(perturbed_image[0].data.detach().cpu().numpy().transpose(1,2,0))\nplt.savefig(\"figures/attacked.png\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = model(sign_data_grad)\noutput.max(1, keepdim=True)[1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Port data to GPU","metadata":{}},{"cell_type":"code","source":"# uploading data to the GPU\ntrain_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)\ntest_loader = DeviceDataLoader(test_loader, device)","metadata":{"execution":{"iopub.execute_input":"2020-08-28T14:07:53.879674Z","iopub.status.busy":"2020-08-28T14:07:53.878011Z","iopub.status.idle":"2020-08-28T14:07:53.880684Z","shell.execute_reply":"2020-08-28T14:07:53.881216Z"},"papermill":{"duration":0.02807,"end_time":"2020-08-28T14:07:53.88133","exception":false,"start_time":"2020-08-28T14:07:53.85326","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"markdown","source":"## Convo model (258 x 258)","metadata":{}},{"cell_type":"code","source":"# extending the original model by adding neural network architecture\nclass FruitRecognitionModel(ImageClassificationBase):\n    def __init__(self, in_size, out_size):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3)\n        self.conv2 = nn.Conv2d(32, 64, 3)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(1032256, 128)\n        self.fc2 = nn.Linear(128, out_size)\n        \n    def forward(self, x):\n        # Pass data through conv1\n        x = self.conv1(x)\n        # Use the rectified-linear activation function over x\n        x = F.relu(x)\n\n        x = self.conv2(x)\n        x = F.relu(x)\n\n        # Run max pooling over x\n        x = F.max_pool2d(x, 2)\n        # Pass data through dropout1\n        x = self.dropout1(x)\n        # Flatten x with start_dim=1\n        x = torch.flatten(x, 1)\n        # Pass data through fc1\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n\n        # Apply softmax to x\n        output = F.log_softmax(x, dim=1)\n        return output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convo model (128 x 128)","metadata":{}},{"cell_type":"code","source":"# extending the original model by adding neural network architecture\nclass FruitRecognitionModel(ImageClassificationBase):\n    def __init__(self, in_size, out_size):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3)\n        self.conv2 = nn.Conv2d(32, 64, 3)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(246016, 128)\n        self.fc2 = nn.Linear(128, out_size)\n        \n    def forward(self, x):\n        # Pass data through conv1\n        x = self.conv1(x)\n        # Use the rectified-linear activation function over x\n        x = F.relu(x)\n\n        x = self.conv2(x)\n        x = F.relu(x)\n\n        # Run max pooling over x\n        x = F.max_pool2d(x, 2)\n        # Pass data through dropout1\n        x = self.dropout1(x)\n        # Flatten x with start_dim=1\n        x = torch.flatten(x, 1)\n        # Pass data through fc1\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n\n        # Apply softmax to x\n        output = F.log_softmax(x, dim=1)\n        return output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convo model (64 x 64)","metadata":{}},{"cell_type":"code","source":"# extending the original model by adding neural network architecture\nclass FruitRecognitionModel(ImageClassificationBase):\n    def __init__(self, in_size, out_size):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3)\n        self.conv2 = nn.Conv2d(32, 64, 3)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(57600, 128)\n        self.fc2 = nn.Linear(128, out_size)\n        \n    def forward(self, x):\n        # Pass data through conv1\n        x = self.conv1(x)\n        # Use the rectified-linear activation function over x\n        x = F.relu(x)\n\n        x = self.conv2(x)\n        x = F.relu(x)\n\n        # Run max pooling over x\n        x = F.max_pool2d(x, 2)\n        # Pass data through dropout1\n        x = self.dropout1(x)\n        # Flatten x with start_dim=1\n        x = torch.flatten(x, 1)\n        # Pass data through fc1\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n\n        # Apply softmax to x\n        output = F.log_softmax(x, dim=1)\n        return output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Five-layer linear model","metadata":{}},{"cell_type":"code","source":"# extending the original model by adding neural network architecture\nclass FruitRecognitionModel(ImageClassificationBase):\n    def __init__(self, in_size, out_size):\n        super().__init__()\n        self.linear1 = nn.Linear(in_size, 1024)\n        self.linear2 = nn.Linear(1024, 512)\n        self.linear3 = nn.Linear(512, 128)\n        self.linear4 = nn.Linear(128, 32)\n        self.linear5 = nn.Linear(32, out_size)\n        \n    def forward(self, xb):\n        # Flatten images into vectors\n        out = xb.view(xb.size(0), -1)\n        # Apply layers & activation functions\n        out = self.linear1(out)\n        out = F.relu(out)\n        out = self.linear2(out)\n        out = F.relu(out)\n        out = self.linear3(out)\n        out = F.relu(out)\n        out = self.linear4(out)\n        out = F.relu(out)\n        out = self.linear5(out)\n        return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convo model (padded 264 x 264)","metadata":{}},{"cell_type":"code","source":"# extending the original model by adding neural network architecture\nclass FruitRecognitionModel(ImageClassificationBase):\n    def __init__(self, in_size, out_size):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3)\n        self.conv2 = nn.Conv2d(32, 64, 3)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(1081600, 128)\n        self.fc2 = nn.Linear(128, out_size)\n        \n    def forward(self, x):\n        # Pass data through conv1\n        x = self.conv1(x)\n        # Use the rectified-linear activation function over x\n        x = F.relu(x)\n\n        x = self.conv2(x)\n        x = F.relu(x)\n\n        # Run max pooling over x\n        x = F.max_pool2d(x, 2)\n        # Pass data through dropout1\n        x = self.dropout1(x)\n        # Flatten x with start_dim=1\n        x = torch.flatten(x, 1)\n        # Pass data through fc1\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n\n        # Apply softmax to x\n        output = F.log_softmax(x, dim=1)\n        return output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train model","metadata":{}},{"cell_type":"code","source":"pixels = 258","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# determine the size of the input (image tensor size) and\n# output (number of classes)\ninput_size = 3*pixels*pixels\noutput_size = len(classes)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# porting the model to the GPU\nmodel = to_device(FruitRecognitionModel(input_size, output_size), device)\nto_device(model, device)","metadata":{"execution":{"iopub.execute_input":"2020-08-28T14:07:54.039886Z","iopub.status.busy":"2020-08-28T14:07:54.038989Z","iopub.status.idle":"2020-08-28T14:07:54.394827Z","shell.execute_reply":"2020-08-28T14:07:54.396236Z"},"papermill":{"duration":0.396674,"end_time":"2020-08-28T14:07:54.396428","exception":false,"start_time":"2020-08-28T14:07:53.999754","status":"completed"},"scrolled":true,"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# check the accuracy and losses on the validation sample\nhistory = [evaluate(model, val_loader)]\nhistory","metadata":{"execution":{"iopub.execute_input":"2020-08-28T14:07:54.479646Z","iopub.status.busy":"2020-08-28T14:07:54.475237Z","iopub.status.idle":"2020-08-28T14:08:04.486409Z","shell.execute_reply":"2020-08-28T14:08:04.485881Z"},"papermill":{"duration":10.05415,"end_time":"2020-08-28T14:08:04.486519","exception":false,"start_time":"2020-08-28T14:07:54.432369","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# we train the model, choosing empirically the optimal step and the number of epochs\nhistory += fit(10, 0.01, model, train_loader, val_loader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build a graph of losses\nplot_losses(history)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build a graph of accuracy\nplot_accuracies(history)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test model","metadata":{}},{"cell_type":"code","source":"# check the model on a test sample\nevaluate(model, test_loader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save model","metadata":{}},{"cell_type":"code","source":"# save the model\ntorch.save(model.state_dict(), '258-pixels-8-bits-convo.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attack padded model","metadata":{}},{"cell_type":"code","source":"data_dir = 'C:\\\\Users\\\\nguye\\\\OneDrive\\\\Documents\\\\0.05 new model'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reduce the original images to one size and transform them into tensors\ndataset = ImageFolder(data_dir, composed)\nprint(\"There are {} images.\".format(len(dataset)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_size = 10000\ntest_size = 7000\ntrain_size = len(dataset) - val_size - test_size\n\ntrain_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\nlen(train_ds), len(val_ds), len(test_ds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set the batch size and create loaders\nbatch_size = 20\ntrain_loader = DataLoader(train_ds, batch_size, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size*2, pin_memory=True)\ntest_loader = DataLoader(test_ds, batch_size*2, pin_memory=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# uploading data to the GPU\ntrain_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)\ntest_loader = DeviceDataLoader(test_loader, device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(model, test_loader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attack convo model","metadata":{}},{"cell_type":"code","source":"data_dir = 'C:\\\\Users\\\\nguye\\\\OneDrive\\\\Documents\\\\0.05 new model'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pixels = 258","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reduce the original images to one size and transform them into tensors\ndsize = (pixels, pixels)\ncomposed = Compose([Resize(dsize), ToTensor()])\n\ndataset = ImageFolder(data_dir, composed)\nprint(\"There are {} images.\".format(len(dataset)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_size = 10000\ntest_size = 7000\ntrain_size = len(dataset) - val_size - test_size\n\ntrain_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\nlen(train_ds), len(val_ds), len(test_ds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set the batch size and create loaders\nbatch_size = 20\ntrain_loader = DataLoader(train_ds, batch_size, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size*2, pin_memory=True)\ntest_loader = DataLoader(test_ds, batch_size*2, pin_memory=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# uploading data to the GPU\ntrain_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)\ntest_loader = DeviceDataLoader(test_loader, device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_size = 3*258*258","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extending the original model by adding neural network architecture\nclass FruitRecognitionModel(ImageClassificationBase):\n    def __init__(self, in_size, out_size):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3)\n        self.conv2 = nn.Conv2d(32, 64, 3)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(1032256, 128)\n        self.fc2 = nn.Linear(128, out_size)\n        \n    def forward(self, x):\n        # Pass data through conv1\n        x = self.conv1(x)\n        # Use the rectified-linear activation function over x\n        x = F.relu(x)\n\n        x = self.conv2(x)\n        x = F.relu(x)\n\n        # Run max pooling over x\n        x = F.max_pool2d(x, 2)\n        # Pass data through dropout1\n        x = self.dropout1(x)\n        # Flatten x with start_dim=1\n        x = torch.flatten(x, 1)\n        # Pass data through fc1\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n\n        # Apply softmax to x\n        output = F.log_softmax(x, dim=1)\n        return output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# porting the model to the GPU\nmodel = to_device(FruitRecognitionModel(input_size, out_size=output_size), device)\nto_device(model, device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_model = 'new-model.pth'\nmodel.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(model, test_loader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Attack normal & padded models","metadata":{}},{"cell_type":"code","source":"# let's look at the resulting classes from directories\ndata_dir = 'C:\\\\Users\\\\nguye\\\\OneDrive\\\\Documents\\\\Fruits'\nclasses = os.listdir(data_dir)\nprint(\"There are {} fruits.\".format(len(classes)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reduce the original images to one size and transform them into tensors\npixels = 258\ndsize = (pixels, pixels)\ncomposed = Compose([Resize(dsize), ToTensor()])\n\ndataset = ImageFolder(data_dir, composed)\nprint(\"There are {} images.\".format(len(dataset)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create training, validation and test sets\nval_size = 10500\ntest_size = 7000\ntrain_size = len(dataset) - val_size - test_size\n\ntrain_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\nlen(train_ds), len(val_ds), len(test_ds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(target_model, defense_model, device, test_loader, epsilon):\n    # Accuracy counter\n    target_correct = 0\n    defense_correct = 0\n\n    # Loop over all examples in test set\n    for data, target in test_loader:\n\n        # Send the data and label to the device\n        data, target = data.to(device), target.to(device)\n\n        # Remember gradients\n        data.requires_grad = True\n\n        # Forward pass the data through the model\n        output = model(data)\n        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        \n        # If the initial prediction is wrong, dont bother attacking, just move on\n        if init_pred.item() != target.item():\n            continue\n\n        # Calculate the loss\n        loss = F.nll_loss(output, target)\n\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect datagrad\n        data_grad = data.grad.data\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n\n        # Re-classify the perturbed image\n        target_output = target_model(perturbed_data)\n        \n        # Defense model\n        perturbed_data = ToPILImage()(perturbed_data[0]).convert(\"RGB\")\n        perturbed_data = composed(perturbed_data).unsqueeze(0).to(device='cuda')\n        defense_output = defense_model(perturbed_data)\n\n        # Check for success\n        target_pred = target_output.max(1, keepdim=True)[1]\n        defense_pred = defense_output.max(1, keepdim=True)[1]\n        \n        if target_pred.item() == target.item():\n            target_correct += 1\n        if defense_pred.item() == target.item():\n            defense_correct += 1\n\n    # Calculate final accuracy for this epsilon\n    target_acc = target_correct/float(len(test_loader))\n    defense_acc = defense_correct/float(len(test_loader))\n    print(\"Epsilon: {}\\tTarget Accuracy = {} / {} = {}\".format(epsilon, target_correct, len(test_loader), target_acc))\n    print(\"Epsilon: {}\\tDefense Accuracy = {} / {} = {}\".format(epsilon, defense_correct, len(test_loader), defense_acc))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epsilons = [.01, .02, .05]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# porting the model to the GPU\nmodel = to_device(FruitRecognitionModel(input_size, output_size), device)\nto_device(model, device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_model = 'new-model.pth'\nmodel.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# porting the model to the GPU\nmodel1 = to_device(FruitRecognitionModel(input_size, output_size), device)\nto_device(model1, device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_model = '264-pixels-resized-padded-convo.pth'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_model = '258-pixels-8-bits-convo.pth'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = DataLoader(test_ds, batch_size=1, num_workers=4, pin_memory=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run test for each epsilon\nfor eps in epsilons:\n    test(model, model, device, test_loader, eps)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test(model, model1, device, test_loader, 0.05)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Attack normal & bit-depth model","metadata":{}},{"cell_type":"code","source":"# Reduce bit-depth\nbits = 8\n\nclass BitDepthReduction:\n    def __call__(self, image):\n        image = TF.pil_to_tensor(image).float() / 255\n        return torch.round(image * (2^bits - 1))/(2^bits - 1) * 255\n\ncomposed = Compose([Resize((pixels, pixels)), BitDepthReduction()])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(target_model, defense_model, device, test_loader, epsilon):\n    # Accuracy counter\n    target_correct = 0\n    defense_correct = 0\n\n    # Loop over all examples in test set\n    for data, target in test_loader:\n\n        # Send the data and label to the device\n        data, target = data.to(device), target.to(device)\n\n        # Remember gradients\n        data.requires_grad = True\n\n        # Forward pass the data through the model\n        output = model(data)\n        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        \n        # If the initial prediction is wrong, dont bother attacking, just move on\n        if init_pred.item() != target.item():\n            continue\n\n        # Calculate the loss\n        loss = F.nll_loss(output, target)\n\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect datagrad\n        data_grad = data.grad.data\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n\n        # Re-classify the perturbed image\n        target_output = target_model(perturbed_data)\n        \n        # Defense model\n        perturbed_data = ToPILImage()(perturbed_data[0]).convert(\"RGB\")\n        perturbed_data = composed(perturbed_data).unsqueeze(0).to(device='cuda')\n        defense_output = defense_model(perturbed_data)\n\n        # Check for success\n        target_pred = target_output.max(1, keepdim=True)[1]\n        defense_pred = defense_output.max(1, keepdim=True)[1]\n        \n        if target_pred.item() == target.item():\n            target_correct += 1\n        if defense_pred.item() == target.item():\n            defense_correct += 1\n\n    # Calculate final accuracy for this epsilon\n    target_acc = target_correct/float(len(test_loader))\n    defense_acc = defense_correct/float(len(test_loader))\n    print(\"Epsilon: {}\\tTarget Accuracy = {} / {} = {}\".format(epsilon, target_correct, len(test_loader), target_acc))\n    print(\"Epsilon: {}\\tDefense Accuracy = {} / {} = {}\".format(epsilon, defense_correct, len(test_loader), defense_acc))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epsilons = [.03, .04, .06]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = DataLoader(test_ds, batch_size=1, num_workers=4, pin_memory=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_model = 'new-model.pth'\nmodel.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_model = '258-pixels-8-bits-convo.pth'\nmodel.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# 8 bits with 255 RGB scale\n# Run test for each epsilon\nfor eps in epsilons:\n    test(model, model, device, test_loader, eps)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# 20 bits\n# Run test for each epsilon\nfor eps in epsilons:\n    test(model, model, device, test_loader, eps)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# 10 bits\n# Run test for each epsilon\nfor eps in epsilons:\n    test(model, model, device, test_loader, eps)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# 6 bits\n# Run test for each epsilon\nfor eps in epsilons:\n    test(model, model, device, test_loader, eps)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# 5 bits\n# Run test for each epsilon\nfor eps in epsilons:\n    test(model, model, device, test_loader, eps)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# 1 bit\n# Run test for each epsilon\nfor eps in epsilons:\n    test(model, model, device, test_loader, eps)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## k-means clustering with bit-depth","metadata":{}},{"cell_type":"code","source":"def k_means_clustering(data, no_of_clusters, centroids=None, maximum_counter=300, tolerance=1e-9, \\\n                       print_output=10):    \n    # Initialise centroids if blank:\n    if centroids is None:\n        centroids = np.random.normal(0, 1, size=(no_of_clusters, data.shape[-1]))\n    else:\n        centroids = centroids.astype(np.float)\n    \n    for N in range(maximum_counter):\n        if N > 0:\n            L_old = L_new\n        # The assignments:\n        assignments = np.zeros((data.shape[0],no_of_clusters))\n        \n        # Distance to centroids\n        dist = cdist(data, centroids)\n        \n        # Assignments:\n        boolean = np.argmin(dist, axis = 1)\n        for i in range(len(boolean)):\n            assignments[i, boolean[i]] = 1\n        \n        # Objective function:\n        L_new = np.sum(assignments*dist**2)\n        \n        # Recalculate centroids:\n        for i in np.unique(boolean):\n            centroids[i,:] = np.mean(data[[j == i for j in boolean]], axis = 0)\n        \n        if N > 0:\n            if (L_old - L_new)/L_new <= tolerance:\n                return centroids, assignments\n        \n    return centroids, assignments","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nno_of_bits = 8\ncentroids, assignments = k_means_clustering(images[0].data.detach().cpu().numpy().transpose(1,2,0).reshape(258*258, 3), 2**no_of_bits)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"approximation = np.matmul(assignments, centroids).reshape(258,258,3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=(15, 5))\n\nfig.add_subplot(1, 2, 1)\nplt.imshow(images[0].data.detach().cpu().numpy().transpose(1,2,0))\nplt.axis('off')\nplt.tight_layout;\n\nfig.add_subplot(1, 2, 2)\nplt.imshow(approximation)\nplt.axis('off')\nplt.tight_layout;\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(target_model, defense_model, device, test_loader, epsilon):\n    # Accuracy counter\n    target_correct = 0\n    defense_correct = 0\n    counter = 0\n\n    # Loop over all examples in test set\n    for data, target in test_loader:        \n        # Send the data and label to the device\n        data, target = data.to(device), target.to(device)\n\n        # Remember gradients\n        data.requires_grad = True\n\n        # Forward pass the data through the model\n        output = model(data)\n        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        \n        # If the initial prediction is wrong, dont bother attacking, just move on\n        if init_pred.item() != target.item():\n            continue\n\n        # Calculate the loss\n        loss = F.nll_loss(output, target)\n\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect datagrad\n        data_grad = data.grad.data\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n\n        # Re-classify the perturbed image\n        target_output = target_model(perturbed_data)\n        \n        # Defense model\n        centroids, assignments = k_means_clustering(perturbed_data[0].data.detach().cpu().numpy().transpose(1,2,0).reshape(258*258,3), 2**no_of_bits)\n        perturbed_data = torch.FloatTensor(np.matmul(assignments, centroids).reshape(1,3,258,258)).to(device='cuda')\n        defense_output = defense_model(perturbed_data)\n\n        # Check for success\n        target_pred = target_output.max(1, keepdim=True)[1]\n        defense_pred = defense_output.max(1, keepdim=True)[1]\n        \n        if target_pred.item() == target.item():\n            target_correct += 1\n        if defense_pred.item() == target.item():\n            defense_correct += 1\n            \n        counter += 1\n        print(counter)\n        if counter == 100:\n            break\n\n    # Calculate final accuracy for this epsilon\n    target_acc = target_correct/float(len(test_loader))\n    defense_acc = defense_correct/float(len(test_loader))\n    print(epsilon)\n    print(target_correct)\n    print(defense_correct)\n    print(\"Epsilon: {}\\tTarget Accuracy = {} / {} = {}\".format(epsilon, target_correct, len(test_loader), target_acc))\n    print(\"Epsilon: {}\\tDefense Accuracy = {} / {} = {}\".format(epsilon, defense_correct, len(test_loader), defense_acc))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = DataLoader(test_ds, batch_size=1, num_workers=4, pin_memory=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# 8 bits with 255 RGB scale\n# Run test for each epsilon\nfor eps in epsilons:\n    test(model, model, device, test_loader, eps)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Single pattern attack","metadata":{}},{"cell_type":"markdown","source":"### New model (258 x 258)","metadata":{}},{"cell_type":"code","source":"%%time\n# we train the model, choosing empirically the optimal step and the number of epochs\nhistory += fit(15, 0.01, model, train_loader, val_loader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build a graph of losses\nplot_losses(history)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build a graph of accuracy\nplot_accuracies(history)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the model on a test sample\nevaluate(model, test_loader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model\ntorch.save(model.state_dict(), 'new-model.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5 layers model (150 x 150)","metadata":{}},{"cell_type":"code","source":"%%time\n# we train the model, choosing empirically the optimal step and the number of epochs\nhistory += fit(13, 0.01, model, train_loader, val_loader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build a graph of losses\nplot_losses(history)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build a graph of accuracy\nplot_accuracies(history)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model\ntorch.save(model.state_dict(), '5-layer-150.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5 layers model (258 x 258)","metadata":{}},{"cell_type":"code","source":"%%time\n# we train the model, choosing empirically the optimal step and the number of epochs\nhistory += fit(13, 0.01, model, train_loader, val_loader)","metadata":{"execution":{"iopub.execute_input":"2020-08-28T14:08:04.533455Z","iopub.status.busy":"2020-08-28T14:08:04.532438Z","iopub.status.idle":"2020-08-28T14:37:43.781616Z","shell.execute_reply":"2020-08-28T14:37:43.782498Z"},"papermill":{"duration":1779.277114,"end_time":"2020-08-28T14:37:43.782717","exception":false,"start_time":"2020-08-28T14:08:04.505603","status":"completed"},"scrolled":true,"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build a graph of losses\nplot_losses(history)","metadata":{"execution":{"iopub.execute_input":"2020-08-28T14:37:43.828101Z","iopub.status.busy":"2020-08-28T14:37:43.826905Z","iopub.status.idle":"2020-08-28T14:37:43.991843Z","shell.execute_reply":"2020-08-28T14:37:43.992368Z"},"papermill":{"duration":0.189767,"end_time":"2020-08-28T14:37:43.992514","exception":false,"start_time":"2020-08-28T14:37:43.802747","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build a graph of accuracy\nplot_accuracies(history)","metadata":{"execution":{"iopub.execute_input":"2020-08-28T14:37:44.045618Z","iopub.status.busy":"2020-08-28T14:37:44.043981Z","iopub.status.idle":"2020-08-28T14:37:44.196544Z","shell.execute_reply":"2020-08-28T14:37:44.197101Z"},"papermill":{"duration":0.182878,"end_time":"2020-08-28T14:37:44.197261","exception":false,"start_time":"2020-08-28T14:37:44.014383","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the model on a test sample\nevaluate(model, test_loader)","metadata":{"execution":{"iopub.execute_input":"2020-08-28T14:37:44.245108Z","iopub.status.busy":"2020-08-28T14:37:44.243421Z","iopub.status.idle":"2020-08-28T14:38:02.233507Z","shell.execute_reply":"2020-08-28T14:38:02.232116Z"},"papermill":{"duration":18.014859,"end_time":"2020-08-28T14:38:02.233623","exception":false,"start_time":"2020-08-28T14:37:44.218764","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model\ntorch.save(model.state_dict(), '5-layer-model.pth')","metadata":{"execution":{"iopub.execute_input":"2020-08-28T14:38:02.285429Z","iopub.status.busy":"2020-08-28T14:38:02.279535Z","iopub.status.idle":"2020-08-28T14:38:02.436736Z","shell.execute_reply":"2020-08-28T14:38:02.43613Z"},"papermill":{"duration":0.182271,"end_time":"2020-08-28T14:38:02.436855","exception":false,"start_time":"2020-08-28T14:38:02.254584","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Universal Perturbations","metadata":{}},{"cell_type":"code","source":"pretrained_model = 'new-model.pth'\nmodel.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"perturbation = Variable(torch.zeros(3, pixels, pixels, requires_grad=True).to(device='cuda'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stepsize = 0.01\noptimiser = optim.SGD(model.parameters(), lr=stepsize, momentum=0.7)\nepsilon = 30\n\ntorch.autograd.set_detect_anomaly(True)\n\nfor data, target in train_loader:\n    data.requires_grad = True\n    perturbation.requires_grad = True\n    perturbation.retain_grad()\n    output_1 = model(data)\n    output_2 = model(data + perturbation.repeat(10,1,1,1))\n\n    loss_1 = F.nll_loss(output_1, target)\n    loss_2 = F.nll_loss(output_2, target)\n    loss = (loss_1 + loss_2) / 2\n    model.zero_grad()\n    loss.backward()\n\n    optimiser.step()\n     \n    perturbation.data = perturbation.data + stepsize * perturbation.grad\n    perturbation.data = (perturbation.data / torch.norm(perturbation.data.view(-1, 66564))\n                        * epsilon)\n    perturbation.data[perturbation.data != perturbation.data] = 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.axis('off')\nplt.imshow(perturbation.data.detach().cpu().numpy().transpose(1, 2, 0))\nplt.savefig(\"figures/universal-pertubation.png\", bbox_inches='tight')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model\ntorch.save(model.state_dict(), '258-pixels-convo-ad-trained2.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model\ntorch.save(model.state_dict(), '258-pixels-convo-ad-trained.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataiter = iter(test_loader)\nimages, labels = next(dataiter)\n# Show a normal image\nplt.axis('off')\nplt.imshow(images[0].data.detach().cpu().numpy().transpose(1,2,0))\nplt.savefig(\"figures/before.png\", bbox_inches='tight')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show an altered image\nplt.axis('off')\nplt.imshow((images[0]+perturbation.data).data.detach().cpu().numpy().transpose(1,2,0))\nplt.savefig(\"figures/after.png\", bbox_inches='tight')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"test_loader = DataLoader(test_ds, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = DeviceDataLoader(test_loader, device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_accuracy(model, test_loader, perturbation):\n    # Accuracy counter\n    correct = 0\n    adv_examples = []\n    \n    # Loop over all examples in test set\n    for data, target in test_loader:\n        # Forward pass the data through the model\n        perturbed_data = data + perturbation\n        output = model(perturbed_data)\n        \n        # Check for success\n        final_pred = output.max(1, keepdim=True)[1][0]\n        \n        if final_pred.item() == target.item():\n            correct += 1\n        \n    # Calculate final accuracy for this epsilon\n    final_acc = correct/float(len(test_loader))\n    print(\"Test Accuracy = {} / {} = {}\".format(correct, len(test_loader), final_acc))\n\n    return final_acc, adv_examples","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = torch.zeros(3, 258, 258).to(device='cuda')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_1, ex_1 = test_accuracy(model, test_loader, x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_2, ex_2 = test_accuracy(model, test_loader, perturbation)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_model = 'new-model.pth'\nmodel.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_3, ex_3 = test_accuracy(model, test_loader, perturbation)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PyTorch attack","metadata":{}},{"cell_type":"markdown","source":"## Save attacked images","metadata":{}},{"cell_type":"code","source":"# FGSM attack code\ndef fgsm_attack(image, epsilon, data_grad):\n    # Collect the element-wise sign of the data gradient\n    sign_data_grad = data_grad.sign()\n    # Create the perturbed image by adjusting each pixel of the input image\n    perturbed_image = image + epsilon*sign_data_grad\n    # Adding clipping to maintain [0,1] range\n    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n    # Return the perturbed image\n    return perturbed_image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_model = 'new-model.pth'\nmodel.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainloader_1 = torch.utils.data.DataLoader(dataset, batch_size=1, num_workers=2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def genadversarial(model, device, test_loader):\n    i = 0\n    for data, target in test_loader:\n        # Send the data and label to the device\n        data, target = data.to(device), target.to(device)\n    \n        data.requires_grad = True\n\n        # Forward pass the data through the model\n        output = model(data)\n\n        # Calculate the loss\n        loss = F.nll_loss(output, target)\n\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect datagrad\n        data_grad = data.grad.data\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data, 0.05, data_grad)\n        plt.imsave('0.05 new model\\\\{}\\\\{}.png'.format(classes[int(target)],i), perturbed_data.detach().cpu().numpy()[0].transpose(1,2,0))\n        i+=1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ngenadversarial(model, device, trainloader_1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train model again","metadata":{}},{"cell_type":"code","source":"data_dir = 'C:\\\\Users\\\\nguye\\\\OneDrive\\\\Documents\\\\Fruits (2)'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reduce the original images to one size and transform them into tensors\ndataset = ImageFolder(data_dir, composed)\nprint(\"There are {} images.\".format(len(dataset)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create training and validation samples from the initial dataset\ntorch.manual_seed(43)\n\nval_size = 20000\ntest_size = 14000\ntrain_size = len(dataset) - val_size - test_size\n\ntrain_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\nlen(train_ds), len(val_ds), len(test_ds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set the batch size and create loaders for the resulting samples using DataLoader\nbatch_size = 32\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_ds, batch_size*2, num_workers=4, pin_memory=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# porting the model to the GPU\nmodel = to_device(FruitRecognitionModel(input_size, out_size=output_size), device)\nto_device(model, device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# uploading data to the GPU\ntrain_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)\ntest_loader = DeviceDataLoader(test_loader, device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the accuracy and losses on the validation sample\nhistory = [evaluate(model, val_loader)]\nhistory","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# we train the model, choosing empirically the optimal step and the number of epochs\nhistory += fit(10, 0.01, model, train_loader, val_loader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build a graph of losses\nplot_losses(history)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build a graph of accuracy\nplot_accuracies(history)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the model on a test sample\nevaluate(model, test_loader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model\ntorch.save(model.state_dict(), '5-layer-model-robust.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Attack robust model","metadata":{}},{"cell_type":"code","source":"data_dir = 'C:\\\\Users\\\\nguye\\\\OneDrive\\\\Documents\\\\Fruits'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = ImageFolder(data_dir, composed)\nprint(\"There are {} images.\".format(len(dataset)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_size = 10500\ntest_size = 7000\ntrain_size = len(dataset) - val_size - test_size\n\ntrain_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\nlen(train_ds), len(val_ds), len(test_ds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = DeviceDataLoader(test_loader, device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epsilons = [0, .01, .02, .03, .04, .05, .06]\npretrained_model = '5-layer-model-robust.pth'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = DataLoader(test_ds, batch_size=1, num_workers=4, pin_memory=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test( model, device, test_loader, epsilon ):\n\n    # Accuracy counter\n    correct = 0\n    adv_examples = []\n\n    # Loop over all examples in test set\n    for data, target in test_loader:\n\n        # Send the data and label to the device\n        data, target = data.to(device), target.to(device)\n\n        # Set requires_grad attribute of tensor. Important for Attack\n        data.requires_grad = True\n\n        # Forward pass the data through the model\n        output = model(data)\n        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        \n        # If the initial prediction is wrong, dont bother attacking, just move on\n        if init_pred.item() != target.item():\n            continue\n\n        # Calculate the loss\n        loss = F.nll_loss(output, target)\n\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect datagrad\n        data_grad = data.grad.data\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n\n        # Re-classify the perturbed image\n        output = model(perturbed_data)\n\n        # Check for success\n        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        if final_pred.item() == target.item():\n            correct += 1\n            # Special case for saving 0 epsilon examples\n            if (epsilon == 0) and (len(adv_examples) < 5):\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n        else:\n            # Save some adv examples for visualization later\n            if len(adv_examples) < 5:\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n\n    # Calculate final accuracy for this epsilon\n    final_acc = correct/float(len(test_loader))\n    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n\n    # Return the accuracy and an adversarial example\n    return final_acc, adv_examples","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, device, test_loader, eps)\n    accuracies.append(acc)\n    examples.append(ex)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .065, step=0.01))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Rotation","metadata":{}},{"cell_type":"code","source":"epsilons = [0, .01, .02, .03, .04, .05, .06]\npretrained_model = \"fruit-recognition-model.pth\"\nuse_cuda=True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.utils import _pair, _quadruple\n\n\nclass MedianPool2d(nn.Module):\n    \"\"\" Median pool (usable as median filter when stride=1) module.\n    \n    Args:\n         kernel_size: size of pooling kernel, int or 2-tuple\n         stride: pool stride, int or 2-tuple\n         padding: pool padding, int or 4-tuple (l, r, t, b) as in pytorch F.pad\n         same: override padding and enforce same padding, boolean\n    \"\"\"\n    def __init__(self, kernel_size=3, stride=1, padding=0, same=False):\n        super(MedianPool2d, self).__init__()\n        self.k = _pair(kernel_size)\n        self.stride = _pair(stride)\n        self.padding = _quadruple(padding)  # convert to l, r, t, b\n        self.same = same\n\n    def _padding(self, x):\n        if self.same:\n            ih, iw = x.size()[2:]\n            if ih % self.stride[0] == 0:\n                ph = max(self.k[0] - self.stride[0], 0)\n            else:\n                ph = max(self.k[0] - (ih % self.stride[0]), 0)\n            if iw % self.stride[1] == 0:\n                pw = max(self.k[1] - self.stride[1], 0)\n            else:\n                pw = max(self.k[1] - (iw % self.stride[1]), 0)\n            pl = pw // 2\n            pr = pw - pl\n            pt = ph // 2\n            pb = ph - pt\n            padding = (pl, pr, pt, pb)\n        else:\n            padding = self.padding\n        return padding\n    \n    def forward(self, x):\n        # using existing pytorch functions and tensor ops so that we get autograd, \n        # would likely be more efficient to implement from scratch at C/Cuda level\n        x = F.pad(x, self._padding(x), mode='reflect')\n        x = x.unfold(2, self.k[0], self.stride[0]).unfold(3, self.k[1], self.stride[1])\n        x = x.contiguous().view(x.size()[:4] + (-1,)).median(dim=-1)[0]\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m = MedianPool2d()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test( model, device, test_loader, epsilon ):\n\n    # Accuracy counter\n    correct = 0\n    adv_examples = []\n\n    # Loop over all examples in test set\n    for data, target in test_loader:\n\n        # Send the data and label to the device\n        data, target = data.to(device), target.to(device)\n\n        # Set requires_grad attribute of tensor. Important for Attack\n        data.requires_grad = True\n\n        # Forward pass the data through the model\n        output = model(data)\n        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        \n        # If the initial prediction is wrong, dont bother attacking, just move on\n        if init_pred.item() != target.item():\n            continue\n\n        # Calculate the loss\n        loss = F.nll_loss(output, target)\n\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect datagrad\n        data_grad = data.grad.data\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n\n        perturbed_data = tfms(perturbed_data)\n        \n        # Re-classify the perturbed image\n        output = model(perturbed_data)\n        \n        # Check for success\n        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        if final_pred.item() == target.item():\n            correct += 1\n            # Special case for saving 0 epsilon examples\n            if (epsilon == 0) and (len(adv_examples) < 5):\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n        else:\n            # Save some adv examples for visualization later\n            if len(adv_examples) < 5:\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n\n    # Calculate final accuracy for this epsilon\n    final_acc = correct/float(len(test_loader))\n    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n\n    # Return the accuracy and an adversarial example\n    return final_acc, adv_examples","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms.functional as TF\n\ntfms = Compose([torchvision.transforms.Resize(250), torchvision.transforms.Pad(4)])\n\ndataloader_iterator = iter(test_loader)\nfor i in range(1):\n    try:\n        data, target = next(dataloader_iterator)\n    except StopIteration:\n        dataloader_iterator = iter(dataloader)\n        data, target = next(dataloader_iterator)\n    data = tfms(data)\n    print(plt.imshow(data[0].detach().cpu().numpy().transpose(1,2,0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, device, test_loader, eps)\n    accuracies.append(acc)\n    examples.append(ex)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .065, step=0.01))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normal FGSM attack","metadata":{}},{"cell_type":"code","source":"epsilons = [0, .01, .02, .03, .04, .05, .06]\npretrained_model = \"264-pixels-resized-padded-convo.pth\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(model, device, test_loader, epsilon):\n    # Accuracy counter\n    correct = 0\n    adv_examples = []\n\n    # Loop over all examples in test set\n    for data, target in test_loader:\n\n        # Send the data and label to the device\n        data, target = data.to(device), target.to(device)\n\n        # Set requires_grad attribute of tensor. Important for Attack\n        data.requires_grad = True\n\n        # Forward pass the data through the model\n        output = model(data)\n        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        \n        print(\"done\")\n        \n        # If the initial prediction is wrong, dont bother attacking, just move on\n        if init_pred.item() != target.item():\n            continue\n\n        # Calculate the loss\n        loss = F.nll_loss(output, target)\n\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect datagrad\n        data_grad = data.grad.data\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n\n        # Re-classify the perturbed image\n        output = model(perturbed_data)\n\n        # Check for success\n        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        if final_pred.item() == target.item():\n            correct += 1\n            # Special case for saving 0 epsilon examples\n            if (epsilon == 0) and (len(adv_examples) < 5):\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n        else:\n            # Save some adv examples for visualization later\n            if len(adv_examples) < 5:\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n\n    # Calculate final accuracy for this epsilon\n    final_acc = correct/float(len(test_loader))\n    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n\n    # Return the accuracy and an adversarial example\n    return final_acc, adv_examples","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = DataLoader(test_ds, batch_size=1, num_workers=4, pin_memory=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, device, test_loader, eps)\n    accuracies.append(acc)\n    examples.append(ex)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .065, step=0.01))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot several examples of adversarial samples at each epsilon\ncnt = 0\nplt.figure(figsize=(11,10))\nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        orig = classes[orig]\n        adv = classes[adv]\n        plt.title(\"{} -> {}\".format(orig, adv))\n        plt.imshow(ex.transpose(1, 2, 0))\nplt.tight_layout()\nplt.savefig('figures/attack1.png')\nplt.show()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Attack new model","metadata":{}},{"cell_type":"code","source":"# FGSM attack code\ndef fgsm_attack(image, epsilon, data_grad):\n    # Collect the element-wise sign of the data gradient\n    sign_data_grad = data_grad.sign()\n    # Create the perturbed image by adjusting each pixel of the input image\n    perturbed_image = image + epsilon*sign_data_grad\n    # Adding clipping to maintain [0,1] range\n    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n    # Return the perturbed image\n    return perturbed_image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epsilons = [0, .01, .02, .03, .04, .05, .06]\npretrained_model = \"new-model.pth\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test( model, device, test_loader, epsilon ):\n    # Accuracy counter\n    correct = 0\n    adv_examples = []\n\n    # Loop over all examples in test set\n    for data, target in test_loader:\n\n        # Send the data and label to the device\n        data, target = data.to(device), target.to(device)\n\n        # Set requires_grad attribute of tensor. Important for Attack\n        data.requires_grad = True\n\n        # Forward pass the data through the model\n        output = model(data)\n        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        \n        # If the initial prediction is wrong, dont bother attacking, just move on\n        if init_pred.item() != target.item():\n            continue\n\n        # Calculate the loss\n        loss = F.nll_loss(output, target)\n\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect datagrad\n        data_grad = data.grad.data\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n\n        # Re-classify the perturbed image\n        output = model(perturbed_data)\n\n        # Check for success\n        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        if final_pred.item() == target.item():\n            correct += 1\n            # Special case for saving 0 epsilon examples\n            if (epsilon == 0) and (len(adv_examples) < 5):\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n        else:\n            # Save some adv examples for visualization later\n            if len(adv_examples) < 5:\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n\n    # Calculate final accuracy for this epsilon\n    final_acc = correct/float(len(test_loader))\n    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n\n    # Return the accuracy and an adversarial example\n    return final_acc, adv_examples","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = DataLoader(test_ds, batch_size=1, num_workers=4, pin_memory=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, device, test_loader, eps)\n    accuracies.append(acc)\n    examples.append(ex)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .065, step=0.01))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Padding","metadata":{}},{"cell_type":"code","source":"tfms = Compose([torchvision.transforms.Resize(256), torchvision.transforms.Pad(1)])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test( model, device, test_loader, epsilon ):\n\n    # Accuracy counter\n    correct = 0\n    adv_examples = []\n\n    # Loop over all examples in test set\n    for data, target in test_loader:\n\n        # Send the data and label to the device\n        data, target = data.to(device), target.to(device)\n\n        # Set requires_grad attribute of tensor. Important for Attack\n        data.requires_grad = True\n\n        # Forward pass the data through the model\n        output = model(data)\n        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        \n        # If the initial prediction is wrong, dont bother attacking, just move on\n        if init_pred.item() != target.item():\n            continue\n\n        # Calculate the loss\n        loss = F.cross_entropy(output, target)\n\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect datagrad\n        data_grad = data.grad.data\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n\n        perturbed_data = tfms(perturbed_data)\n        \n        # Re-classify the perturbed image\n        output = model(perturbed_data)\n        \n        # Check for success\n        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        if final_pred.item() == target.item():\n            correct += 1\n            # Special case for saving 0 epsilon examples\n            if (epsilon == 0) and (len(adv_examples) < 5):\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n        else:\n            # Save some adv examples for visualization later\n            if len(adv_examples) < 5:\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n\n    # Calculate final accuracy for this epsilon\n    final_acc = correct/float(len(test_loader))\n    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n\n    # Return the accuracy and an adversarial example\n    return final_acc, adv_examples","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, device, test_loader, eps)\n    accuracies.append(acc)\n    examples.append(ex)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .065, step=0.01))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defence","metadata":{}},{"cell_type":"code","source":"\"\"\"Implementation of sample defense.\nThis defense loads inception resnet v2 checkpoint and classifies all images\nusing loaded checkpoint.\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport random\n\nimport numpy as np\nfrom scipy.misc import imread\n\nimport tensorflow as tf\n\nimport inception_resnet_v2\n\nslim = tf.contrib.slim\n\n\ntf.flags.DEFINE_string(\n    'master', '', 'The address of the TensorFlow master to use.')\n\ntf.flags.DEFINE_string(\n    'checkpoint_path', '', 'Path to checkpoint for inception network.')\n\ntf.flags.DEFINE_string(\n    'input_dir', '', 'Input directory with images.')\n\ntf.flags.DEFINE_string(\n    'output_file', '', 'Output file to save labels.')\n\ntf.flags.DEFINE_integer(\n    'image_width', 299, 'Width of each input images.')\n\ntf.flags.DEFINE_integer(\n    'image_height', 299, 'Height of each input images.')\n\ntf.flags.DEFINE_integer(\n    'batch_size', 16, 'How many images process at one time.')\n\ntf.flags.DEFINE_integer(\n    'image_resize', 331, 'Resize of image size.')\n\nFLAGS = tf.flags.FLAGS\n\n\n\ndef padding_layer_iyswim(inputs, shape, name=None):\n    h_start = shape[0]\n    w_start = shape[1]\n    output_short = shape[2]\n    input_shape = tf.shape(inputs)\n    input_short = tf.reduce_min(input_shape[1:3])\n    input_long = tf.reduce_max(input_shape[1:3])\n    output_long = tf.to_int32(tf.ceil(\n        1. * tf.to_float(output_short) * tf.to_float(input_long) / tf.to_float(input_short)))\n    output_height = tf.to_int32(input_shape[1] >= input_shape[2]) * output_long +\\\n        tf.to_int32(input_shape[1] < input_shape[2]) * output_short\n    output_width = tf.to_int32(input_shape[1] >= input_shape[2]) * output_short +\\\n        tf.to_int32(input_shape[1] < input_shape[2]) * output_long\n    return tf.pad(inputs, tf.to_int32(tf.stack([[0, 0], [h_start, output_height - h_start - input_shape[1]], [w_start, output_width - w_start - input_shape[2]], [0, 0]])), name=name)\n\n\ndef load_images(input_dir, batch_shape):\n    \"\"\"Read png images from input directory in batches.\n    Args:\n      input_dir: input directory\n      batch_shape: shape of minibatch array, i.e. [batch_size, height, width, 3]\n    Yields:\n      filenames: list file names without path of each image\n        Lenght of this list could be less than batch_size, in this case only\n        first few images of the result are elements of the minibatch.\n      images: array with all images from this batch\n    \"\"\"\n    images = np.zeros(batch_shape)\n    filenames = []\n    idx = 0\n    batch_size = batch_shape[0]\n    for filepath in tf.gfile.Glob(os.path.join(input_dir, '*.png')):\n        with tf.gfile.Open(filepath) as f:\n            image = imread(f, mode='RGB').astype(np.float) / 255.0\n        # Images for inception classifier are normalized to be in [-1, 1] interval.\n        images[idx, :, :, :] = image * 2.0 - 1.0\n        filenames.append(os.path.basename(filepath))\n        idx += 1\n        if idx == batch_size:\n            yield filenames, images\n            filenames = []\n            images = np.zeros(batch_shape)\n            idx = 0\n    if idx > 0:\n        yield filenames, images\n\n\ndef main(_):\n    batch_shape = [FLAGS.batch_size, FLAGS.image_height, FLAGS.image_width, 3]\n    num_classes = 1001\n    itr = 30\n\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    with tf.Graph().as_default():\n        # Prepare graph\n        x_input = tf.placeholder(tf.float32, shape=batch_shape)\n        img_resize_tensor = tf.placeholder(tf.int32, [2])\n        x_input_resize = tf.image.resize_images(x_input, img_resize_tensor, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n        shape_tensor = tf.placeholder(tf.int32, [3])\n        padded_input = padding_layer_iyswim(x_input_resize, shape_tensor)\n        # 330 is the last value to keep 8*8 output, 362 is the last value to keep 9*9 output, stride = 32\n        padded_input.set_shape(\n            (FLAGS.batch_size, FLAGS.image_resize, FLAGS.image_resize, 3))\n\n        with slim.arg_scope(inception_resnet_v2.inception_resnet_v2_arg_scope()):\n            _, end_points = inception_resnet_v2.inception_resnet_v2(\n                padded_input, num_classes=num_classes, is_training=False, create_aux_logits=True)\n\n        predicted_labels = tf.argmax(end_points['Predictions'], 1)\n\n        # Run computation\n        saver = tf.train.Saver(slim.get_model_variables())\n        session_creator = tf.train.ChiefSessionCreator(\n            scaffold=tf.train.Scaffold(saver=saver),\n            checkpoint_filename_with_path=FLAGS.checkpoint_path,\n            master=FLAGS.master)\n\n        with tf.train.MonitoredSession(session_creator=session_creator) as sess:\n            with tf.gfile.Open(FLAGS.output_file, 'w') as out_file:\n                for filenames, images in load_images(FLAGS.input_dir, batch_shape):\n                    final_preds = np.zeros(\n                        [FLAGS.batch_size, num_classes, itr])\n                    for j in range(itr):\n                        if np.random.randint(0, 2, size=1) == 1:\n                            images = images[:, :, ::-1, :]\n                        resize_shape_ = np.random.randint(310, 331)\n                        pred, aux_pred = sess.run([end_points['Predictions'], end_points['AuxPredictions']],\n                                                        feed_dict={x_input: images, img_resize_tensor: [resize_shape_]*2,\n                                                                   shape_tensor: np.array([random.randint(0, FLAGS.image_resize - resize_shape_), random.randint(0, FLAGS.image_resize - resize_shape_), FLAGS.image_resize])})\n                        final_preds[..., j] = pred + 0.4 * aux_pred\n                    final_probs = np.sum(final_preds, axis=-1)\n                    labels = np.argmax(final_probs, 1)\n                    for filename, label in zip(filenames, labels):\n                        out_file.write('{0},{1}\\n'.format(filename, label))\n\n\nif __name__ == '__main__':\n    tf.app.run()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 64x64 GAN","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nmanualSeed = 999\n#manualSeed = random.randint(1, 10000) # use if you want new results\nprint(\"Random Seed: \", manualSeed)\nrandom.seed(manualSeed)\ntorch.manual_seed(manualSeed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of workers for dataloader\nworkers = 2\n\n# Batch size during training\nbatch_size = 128\n\n# Number of channels in the training images. For color images this is 3\nnc = 3\n\n# Size of z latent vector (i.e. size of generator input)\nnz = 100\n\n# Size of feature maps in generator\nngf = 64\n\n# Size of feature maps in discriminator\nndf = 64\n\n# Number of training epochs\nnum_epochs = 5\n\n# Learning rate for optimizers\nlr = 0.0002\n\n# Beta1 hyperparam for Adam optimizers\nbeta1 = 0.5\n\n# Number of GPUs available. Use 0 for CPU mode.\nngpu = 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                         shuffle=True, num_workers=workers)\n\n# Decide which device we want to run on\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n\n# Plot some training images\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generator Code\n\nclass Generator(nn.Module):\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        return self.main(input)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the generator\nnetG = Generator(ngpu).to(device)\n\n# Handle multi-gpu if desired\nif (device.type == 'cuda') and (ngpu > 1):\n    netG = nn.DataParallel(netG, list(range(ngpu)))\n\n# Apply the weights_init function to randomly initialize all weights\n#  to mean=0, stdev=0.2.\nnetG.apply(weights_init)\n\n# Print the model\nprint(netG)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, ngpu):\n        super(Discriminator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the Discriminator\nnetD = Discriminator(ngpu).to(device)\n\n# Handle multi-gpu if desired\nif (device.type == 'cuda') and (ngpu > 1):\n    netD = nn.DataParallel(netD, list(range(ngpu)))\n\n# Apply the weights_init function to randomly initialize all weights\n#  to mean=0, stdev=0.2.\nnetD.apply(weights_init)\n\n# Print the model\nprint(netD)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize BCELoss function\ncriterion = nn.BCELoss()\n\n# Create batch of latent vectors that we will use to visualize\n#  the progression of the generator\nfixed_noise = torch.randn(64, nz, 1, 1, device=device)\n\n# Establish convention for real and fake labels during training\nreal_label = 1.\nfake_label = 0.\n\n# Setup Adam optimizers for both G and D\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training GAN","metadata":{}},{"cell_type":"code","source":"pretrained_model = '64-discriminative.pth'\nnetD.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training Loop\n\n# Lists to keep track of progress\nimg_list = []\nG_losses = []\nD_losses = []\niters = 0\n\nprint(\"Starting Training Loop...\")\n# For each epoch\nfor epoch in range(num_epochs):\n    # For each batch in the dataloader\n    for i, data in enumerate(dataloader, 0):\n\n        ############################\n        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n        ###########################\n        ## Train with all-real batch\n        netD.zero_grad()\n        # Format batch\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n        # Forward pass real batch through D\n        output = netD(real_cpu).view(-1)\n        # Calculate loss on all-real batch\n        errD_real = criterion(output, label)\n        # Calculate gradients for D in backward pass\n        errD_real.backward()\n        D_x = output.mean().item()\n\n        ## Train with all-fake batch\n        # Generate batch of latent vectors\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        # Generate fake image batch with G\n        fake = netG(noise)\n        label.fill_(fake_label)\n        # Classify all fake batch with D\n        output = netD(fake.detach()).view(-1)\n        # Calculate D's loss on the all-fake batch\n        errD_fake = criterion(output, label)\n        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        # Compute error of D as sum over the fake and the real batches\n        errD = errD_real + errD_fake\n        # Update D\n        optimizerD.step()\n\n        ############################\n        # (2) Update G network: maximize log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        label.fill_(real_label)  # fake labels are real for generator cost\n        # Since we just updated D, perform another forward pass of all-fake batch through D\n        output = netD(fake).view(-1)\n        # Calculate G's loss based on this output\n        errG = criterion(output, label)\n        # Calculate gradients for G\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        # Update G\n        optimizerG.step()\n\n        # Output training stats\n        if i % 50 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                  % (epoch, num_epochs, i, len(dataloader),\n                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n        # Save Losses for plotting later\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n\n        # Check how the generator is doing by saving G's output on fixed_noise\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n\n        iters += 1","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(G_losses,label=\"G\")\nplt.plot(D_losses,label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%capture\nfig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Grab a batch of real images from the dataloader\nreal_batch = next(iter(dataloader))\n\n# Plot the real images\nplt.figure(figsize=(15,15))\nplt.subplot(1,2,1)\nplt.axis(\"off\")\nplt.title(\"Real Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n\n# Plot the fake images from the last epoch\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.title(\"Fake Images\")\nplt.imshow(np.transpose(img_list[-1],(1,2,0)))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save generative model\ntorch.save(netG.state_dict(), '64-generative.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save generative model\ntorch.save(netD.state_dict(), '64-discriminative.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 128x128 GAN","metadata":{}},{"cell_type":"code","source":"# Number of workers for dataloader\nworkers = 2\n\n# Batch size during training\nbatch_size = 128\n\n# Number of channels in the training images. For color images this is 3\nnc = 3\n\n# Size of z latent vector (i.e. size of generator input)\nnz = 200\n\n# Size of feature maps in generator\nngf = 128\n\n# Size of feature maps in discriminator\nndf = 64\n\n# Number of training epochs\nnum_epochs = 5\n\n# Learning rate for optimizers; 0.0005 is too high\nlr = 0.0002\n\n# Beta1 hyperparam for Adam optimizers\nbeta1 = 0.5\n\n# Number of GPUs available. Use 0 for CPU mode.\nngpu = 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                         shuffle=True, num_workers=workers)\n\n# Decide which device we want to run on\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n\n# Plot some training images\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d(     nz, ngf * 16, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 16),\n            nn.ReLU(True),\n            # state size. (ngf*16) x 4 x 4\n            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            # state size. (ngf*8) x 8 x 8\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            # state size. (ngf*4) x 16 x 16 \n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # state size. (ngf*2) x 32 x 32\n            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # state size. (ngf) x 64 x 64\n            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 128 x 128\n        )\n    \n    def forward(self, input):\n        return self.main(input)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the generator\nnetG = Generator(ngpu).to(device)\n\n# Handle multi-gpu if desired\nif (device.type == 'cuda') and (ngpu > 1):\n    netG = nn.DataParallel(netG, list(range(ngpu)))\n\n# Apply the weights_init function to randomly initialize all weights\n#  to mean=0, stdev=0.2.\nnetG.apply(weights_init)\n\n# Print the model\nprint(netG)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, ngpu):\n        super(Discriminator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is (nc) x 128 x 128\n            nn.Conv2d(nc, ndf, 4, stride=2, padding=1, bias=False), \n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 64 x 64\n            nn.Conv2d(ndf, ndf * 2, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 32 x 32\n            nn.Conv2d(ndf * 2, ndf * 4, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 16 x 16 \n            nn.Conv2d(ndf * 4, ndf * 8, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 8 x 8\n            nn.Conv2d(ndf * 8, ndf * 16, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(ndf * 16),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*16) x 4 x 4\n            nn.Conv2d(ndf * 16, 1, 4, stride=1, padding=0, bias=False),\n            nn.Sigmoid()\n            # state size. 1\n        )\n    \n    def forward(self, input):\n        return self.main(input)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the Discriminator\nnetD = Discriminator(ngpu).to(device)\n\n# Handle multi-gpu if desired\nif (device.type == 'cuda') and (ngpu > 1):\n    netD = nn.DataParallel(netD, list(range(ngpu)))\n\n# Apply the weights_init function to randomly initialize all weights\n#  to mean=0, stdev=0.2.\nnetD.apply(weights_init)\n\n# Print the model\nprint(netD)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize BCELoss function\ncriterion = nn.BCELoss()\n\n# Create batch of latent vectors that we will use to visualize\n#  the progression of the generator\nfixed_noise = torch.randn(64, nz, 1, 1, device=device)\n\n# Establish convention for real and fake labels during training\nreal_label = 1.\nfake_label = 0.\n\n# Setup Adam optimizers for both G and D\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_model = '128-discriminative.pth'\nnetD.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# From: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n# Training Loop\n\n# Lists to keep track of progress\nimg_list = []\nG_losses = []\nD_losses = []\niters = 0\n\nprint(\"Starting Training Loop...\")\n# For each epoch\nfor epoch in range(num_epochs):\n    # For each batch in the dataloader\n    for i, data in enumerate(dataloader, 0):\n\n        ############################\n        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n        ###########################\n        ## Train with all-real batch\n        netD.zero_grad()\n        # Format batch\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n        # Forward pass real batch through D\n        output = netD(real_cpu).view(-1)\n        # Calculate loss on all-real batch\n        errD_real = criterion(output, label)\n        # Calculate gradients for D in backward pass\n        errD_real.backward()\n        D_x = output.mean().item()\n\n        ## Train with all-fake batch\n        # Generate batch of latent vectors\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        # Generate fake image batch with G\n        fake = netG(noise)\n        label.fill_(fake_label)\n        # Classify all fake batch with D\n        output = netD(fake.detach()).view(-1)\n        # Calculate D's loss on the all-fake batch\n        errD_fake = criterion(output, label)\n        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        # Compute error of D as sum over the fake and the real batches\n        errD = errD_real + errD_fake\n        # Update D\n        optimizerD.step()\n\n        ############################\n        # (2) Update G network: maximize log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        label.fill_(real_label)  # fake labels are real for generator cost\n        # Since we just updated D, perform another forward pass of all-fake batch through D\n        output = netD(fake).view(-1)\n        # Calculate G's loss based on this output\n        errG = criterion(output, label)\n        # Calculate gradients for G\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        # Update G\n        optimizerG.step()\n\n        # Output training stats\n        if i % 50 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                  % (epoch, num_epochs, i, len(dataloader),\n                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n        # Save Losses for plotting later\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n\n        # Check how the generator is doing by saving G's output on fixed_noise\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n\n        iters += 1","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(G_losses,label=\"G\")\nplt.plot(D_losses,label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%capture\nfig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Grab a batch of real images from the dataloader\nreal_batch = next(iter(dataloader))\n\n# Plot the real images\nplt.figure(figsize=(15,15))\nplt.subplot(1,2,1)\nplt.axis(\"off\")\nplt.title(\"Real Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n\n# Plot the fake images from the last epoch\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.title(\"Fake Images\")\nplt.imshow(np.transpose(img_list[-1],(1,2,0)))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save generative model\ntorch.save(netG.state_dict(), '128-generative.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save discriminative model\ntorch.save(netD.state_dict(), '128-discriminative.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate a picture close to original picture","metadata":{}},{"cell_type":"code","source":"pretrained_model = '64-pixels-convo.pth'\nmodel.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_model = '64-generative.pth'\nnetG.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = DeviceDataLoader(DataLoader(test_ds, batch_size=1, num_workers=4, pin_memory=True), device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataiter = iter(test_loader)\nimages, labels = next(dataiter)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show a normal image\nplt.axis('off')\nplt.imshow(images[0].data.detach().cpu().numpy().transpose(1,2,0))\nplt.savefig('figures/original guava.png')","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Next image\nimages, labels = next(dataiter)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show a normal image\nplt.axis('off')\nplt.imshow(images[0].data.detach().cpu().numpy().transpose(1,2,0))\n#plt.savefig('figures/original pear.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualise GAN images","metadata":{}},{"cell_type":"code","source":"def train_loop(image, model, loss_fn, R, L): \n    for i in range(R):\n        a = torch.randn(1, nz, 1, 1, device=device)\n        pred = model(a)\n        loss = loss_fn(pred, image)\n        if i == 0:\n            minimum_loss = loss\n            random = a\n        elif loss < minimum_loss:\n            minimum_loss = loss\n            random = a\n        \n    for i in range(L):\n        # Compute prediction and loss\n        random.requires_grad = True\n        random.retain_grad()\n        \n        pred = model(random)\n        loss = loss_fn(pred, image)\n        \n        model.zero_grad()\n        loss.backward(retain_graph=True)\n        \n        random.data -= 0.1*random.grad.data\n    \n    return model(random)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the loss function\nloss_fn = nn.MSELoss()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfig = plt.figure(figsize=(18.5, 10.5))\ncolumns = 7\nrows = 5\n\ndataiter = iter(test_loader)\n\n# ax enables access to manipulate each of subplots\nax = []\n\nfor i in range(rows):\n    # select image\n    data, target = next(dataiter)\n    img = data[0].data.detach().cpu().numpy().transpose(1, 2, 0)\n    # create subplot and append to ax\n    ax.append(fig.add_subplot(rows, columns, (i*columns)+1))\n    # set title\n    ax[0].set_title(\"Original\", fontdict = {'fontsize': 16}, fontproperties=font)\n    # remove axes\n    ax[-1].xaxis.set_visible(False)\n    ax[-1].yaxis.set_visible(False)\n    # put image to display\n    plt.imshow(img)\n    \n    # adversarial images\n    # create subplot and append to ax\n    data, target = data.to(device), target.to(device)\n    # Remember gradients\n    data.requires_grad = True\n    # Forward pass the data through the model\n    output = model(data)\n    # Calculate the loss\n    loss = F.nll_loss(output, target)\n    # Zero all existing gradients\n    model.zero_grad()\n    # Calculate gradients of model in backward pass\n    loss.backward()\n    # Collect datagrad\n    data_grad = data.grad.data\n    # Call FGSM Attack\n    perturbed_data = fgsm_attack(data, 0.05, data_grad)\n    display_perturbed_data = perturbed_data[0].data.detach().cpu().numpy().transpose(1, 2, 0)\n    ax.append(fig.add_subplot(rows, columns, (i*columns)+2))\n    # set title\n    ax[1].set_title(\"Adv\", fontdict = {'fontsize': 16}, fontproperties=font)\n    # remove axes\n    ax[-1].xaxis.set_visible(False)\n    ax[-1].yaxis.set_visible(False)\n    # put image to display\n    plt.imshow(display_perturbed_data)\n    \n    for j in range(1,6):\n        # adversarial images that change L\n        random = train_loop(perturbed_data, netG, loss_fn, 30000, 1000*j)\n        random = random[0].data.detach().cpu().numpy().transpose(1, 2, 0)\n        ax.append(fig.add_subplot(rows, columns, (i*columns)+2+j))\n        # remove axes\n        ax[-1].xaxis.set_visible(False)\n        ax[-1].yaxis.set_visible(False)\n        # put image to display\n        plt.imshow(random)\n\nfor i in range(2,7):\n    ax[i].set_title(\"L = {:,}\".format((i-1)*1000), fontdict = {'fontsize': 16}, fontproperties=font)\n\n# render plot\nplt.show()\n\n# save plot\nfig.savefig('figures/GAN-changes-2.png', dpi=100)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfig = plt.figure(figsize=(18.5, 10.5))\ncolumns = 7\nrows = 5\n\ndataiter = iter(test_loader)\n\n# ax enables access to manipulate each of subplots\nax = []\n\nfor i in range(rows):\n    # select image\n    data, target = next(dataiter)\n    img = data[0].data.detach().cpu().numpy().transpose(1, 2, 0)\n    # create subplot and append to ax\n    ax.append(fig.add_subplot(rows, columns, (i*columns)+1))\n    # set title\n    ax[0].set_title(\"Original\", fontdict = {'fontsize': 16}, fontproperties=font)\n    # remove axes\n    ax[-1].xaxis.set_visible(False)\n    ax[-1].yaxis.set_visible(False)\n    # put image to display\n    plt.imshow(img)\n    \n    # adversarial images\n    # create subplot and append to ax\n    data, target = data.to(device), target.to(device)\n    # Remember gradients\n    data.requires_grad = True\n    # Forward pass the data through the model\n    output = model(data)\n    # Calculate the loss\n    loss = F.nll_loss(output, target)\n    # Zero all existing gradients\n    model.zero_grad()\n    # Calculate gradients of model in backward pass\n    loss.backward()\n    # Collect datagrad\n    data_grad = data.grad.data\n    # Call FGSM Attack\n    perturbed_data = fgsm_attack(data, 0.05, data_grad)\n    display_perturbed_data = perturbed_data[0].data.detach().cpu().numpy().transpose(1, 2, 0)\n    ax.append(fig.add_subplot(rows, columns, (i*columns)+2))\n    # set title\n    ax[1].set_title(\"Adv\", fontdict = {'fontsize': 16}, fontproperties=font)\n    # remove axes\n    ax[-1].xaxis.set_visible(False)\n    ax[-1].yaxis.set_visible(False)\n    # put image to display\n    plt.imshow(display_perturbed_data)\n    \n    for j in range(1,6):\n        # adversarial images that change L\n        random = train_loop(perturbed_data, netG, loss_fn, 10000*j, 5000)\n        random = random[0].data.detach().cpu().numpy().transpose(1, 2, 0)\n        ax.append(fig.add_subplot(rows, columns, (i*columns)+2+j))\n        # remove axes\n        ax[-1].xaxis.set_visible(False)\n        ax[-1].yaxis.set_visible(False)\n        # put image to display\n        plt.imshow(random)\n\nfor i in range(2,7):\n    ax[i].set_title(\"R = {:,}\".format((i-1)*10000), fontdict = {'fontsize': 16}, fontproperties=font)\n\n# render plot\nplt.show()\n\n# save plot\nfig.savefig('figures/GAN-changes-R.png', dpi=100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nrandom = train_loop(images, netG, loss_fn, 30000, 1000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nrandom = train_loop(images, netG, loss_fn, 30000, 5000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.axis('off')\nplt.imshow(random[0].permute(1,2,0).cpu().detach().numpy())\n#plt.savefig('figures/recreated pear.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# See how GAN works with classifier","metadata":{}},{"cell_type":"code","source":"pretrained_model = '64-pixels-convo.pth'\nmodel.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_model = 'new-model.pth'\nmodel.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"success = 0\nfor i in range(100):\n    if torch.max(model(images), dim=1)[1].item() == 3:\n        success+=1\nprint(success)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"success = 0\nfor i in range(100):\n    if torch.max(model(random), dim=1)[1].item() == 3:\n        success+=1\nprint(success)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FGSM attack code\ndef fgsm_attack(image, epsilon, data_grad):\n    # Collect the element-wise sign of the data gradient\n    sign_data_grad = data_grad.sign()\n    # Create the perturbed image by adjusting each pixel of the input image\n    perturbed_image = image + epsilon*sign_data_grad\n    # Adding clipping to maintain [0,1] range\n    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n    # Return the perturbed image\n    return perturbed_image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(target_model, device, test_loader, epsilon):\n    # Accuracy counter\n    target_correct = 0\n    defense_correct = 0\n    i = 0\n\n    # Loop over all examples in test set\n    for data, target in test_loader:\n\n        # Send the data and label to the device\n        data, target = data.to(device), target.to(device)\n\n        # Remember gradients\n        data.requires_grad = True\n\n        # Forward pass the data through the model\n        output = model(data)\n        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        \n        # If the initial prediction is wrong, dont bother attacking, just move on\n        if init_pred.item() != target.item():\n            continue\n\n        # Calculate the loss\n        loss = F.nll_loss(output, target)\n\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect datagrad\n        data_grad = data.grad.data\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n\n        # Re-classify the perturbed image\n        target_output = target_model(perturbed_data)\n        \n        # GAN-Defense model\n        perturbed_data = train_loop(perturbed_data.data, netG, loss_fn, 30000, 1000)\n        defense_output = target_model(perturbed_data)\n\n        # Check for success\n        target_pred = target_output.max(1, keepdim=True)[1]\n        defense_pred = defense_output.max(1, keepdim=True)[1]\n        \n        if target_pred.item() == target.item():\n            target_correct += 1\n        if defense_pred.item() == target.item():\n            defense_correct += 1\n    \n        i += 1\n        if i == 100:\n            break\n        \n    # Calculate final accuracy for this epsilon\n    target_acc = target_correct/float(len(test_loader))\n    defense_acc = defense_correct/float(len(test_loader))\n    print(\"Epsilon: {}\\tTarget Accuracy = {} / {} = {}\".format(epsilon, target_correct, len(test_loader), target_acc))\n    print(\"Epsilon: {}\\tDefense Accuracy = {} / {} = {}\".format(epsilon, defense_correct, len(test_loader), defense_acc))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epsilons = [0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Run test for each epsilon\nfor eps in epsilons:\n    test(model, device, test_loader, eps)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]}]}