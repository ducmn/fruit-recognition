{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-23T15:44:14.767154Z",
     "iopub.status.busy": "2022-03-23T15:44:14.766882Z",
     "iopub.status.idle": "2022-03-23T15:44:14.780604Z",
     "shell.execute_reply": "2022-03-23T15:44:14.779991Z",
     "shell.execute_reply.started": "2022-03-23T15:44:14.767122Z"
    },
    "papermill": {
     "duration": 1.868959,
     "end_time": "2020-08-28T14:03:27.558422",
     "exception": false,
     "start_time": "2020-08-28T14:03:25.689463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n",
    "\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../input/fruit-recognition'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 70549 images.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27ee50e0eb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Max = 258\n",
    "pixels = 64\n",
    "\n",
    "composed = Compose([Resize((pixels, pixels)), ToTensor()])\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(data_dir, composed)\n",
    "print(\"There are {} images.\".format(len(dataset)))\n",
    "\n",
    "# An example image\n",
    "plt.imshow(dataset[6][0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bit-depth reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce bit-depth\n",
    "pixels = 258\n",
    "bits = 20\n",
    "\n",
    "class BitDepthReduction:\n",
    "    def __call__(self, image):\n",
    "        image = TF.pil_to_tensor(image).float() / 255\n",
    "        return torch.round(image * (2^bits - 1))/(2^bits - 1)\n",
    "\n",
    "composed = Compose([Resize((pixels, pixels)), BitDepthReduction()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(data_dir, composed)\n",
    "print(\"There are {} images.\".format(len(dataset)))\n",
    "\n",
    "# An example image\n",
    "plt.imshow(dataset[6][0].permute(1,2,0))\n",
    "plt.savefig('figures/8-bits-apples.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize to [pixel * x, pixel) & pad to pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size & minimum resize size\n",
    "pixels = 264\n",
    "min_resize_ratio = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_resize = np.round_(pixels * min_resize_ratio)\n",
    "print(\"Minimum resized size of image is:\", minimum_resize, \"pixels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomResize:\n",
    "    def __call__(self, image):\n",
    "        new_w = random.randint(minimum_resize, pixels)\n",
    "        new_h = random.randint(minimum_resize, pixels)\n",
    "        return TF.resize(image, [new_w, new_h])\n",
    "\n",
    "class SquarePad:\n",
    "    def __call__(self, image):\n",
    "        W, H = image.size\n",
    "        left = random.randint(0, pixels - W + 1)\n",
    "        top = random.randint(0, pixels - W + 1)\n",
    "        right = pixels - W - left\n",
    "        bottom = pixels - H - top\n",
    "        padding = (left, top, right, bottom)\n",
    "        return TF.pad(image, padding)\n",
    "\n",
    "composed = Compose([RandomResize(), SquarePad(), ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(data_dir, composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad to pixel only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquarePad:\n",
    "    def __call__(self, image):\n",
    "        padding = (3, 3, 3, 3)\n",
    "        return TF.pad(image, padding)\n",
    "\n",
    "composed = Compose([Resize((258, 258)), SquarePad(), ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(data_dir, composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-mean bit-depth reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class k_mean_bit_depth:\n",
    "    def __call__(self, image):\n",
    "        centroids = np.random.normal(0, 1, size=(2**8, 3))\n",
    "        for N in range(100):\n",
    "            if N > 0:\n",
    "                L_old = L_new\n",
    "            # The assignments:\n",
    "            assignments = np.zeros((258**2,2**8))\n",
    "            \n",
    "            image = np.array(image).reshape(258**2,3)\n",
    "            \n",
    "            # Distance to centroids\n",
    "            dist = cdist(image, centroids)\n",
    "\n",
    "            # Assignments:\n",
    "            boolean = np.argmin(dist, axis = 1)\n",
    "            for i in range(len(boolean)):\n",
    "                assignments[i, boolean[i]] = 1\n",
    "\n",
    "            # Objective function:\n",
    "            L_new = np.sum(assignments*dist**2)\n",
    "\n",
    "            # Recalculate centroids:\n",
    "            for i in np.unique(boolean):\n",
    "                centroids[i,:] = np.mean(image[[j == i for j in boolean]], axis = 0)\n",
    "\n",
    "            if N > 0:\n",
    "                if (L_old - L_new)/L_new <= 1e-9:\n",
    "                    return np.matmul(assignments, centroids).reshape(258,258,3)\n",
    "        return np.matmul(assignments, centroids).reshape(258,258,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed = Compose([Resize((258, 258)), k_mean_bit_depth(), ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(data_dir, composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide data into sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training, validation and test sets\n",
    "val_size = 10509\n",
    "test_size = 7000\n",
    "train_size = len(dataset) - val_size - test_size\n",
    "\n",
    "train_ds, val_ds, test_ds = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "print(\"Training set size:\", len(train_ds))\n",
    "print(\"Validation set size:\", len(val_ds))\n",
    "print(\"Test set size:\", len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size & create loaders\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(train_ds, batch_size, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size*2, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size*2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "# Show an image\n",
    "plt.axis('off')\n",
    "plt.imshow(images[0].data.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019262,
     "end_time": "2020-08-28T14:07:53.475947",
     "exception": false,
     "start_time": "2020-08-28T14:07:53.456685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-28T14:07:53.523004Z",
     "iopub.status.busy": "2020-08-28T14:07:53.521972Z",
     "iopub.status.idle": "2020-08-28T14:07:53.527393Z",
     "shell.execute_reply": "2020-08-28T14:07:53.526655Z"
    },
    "papermill": {
     "duration": 0.032211,
     "end_time": "2020-08-28T14:07:53.527549",
     "exception": false,
     "start_time": "2020-08-28T14:07:53.495338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "# Create a base model class (no architecture)\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels) \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    \n",
    "        loss = F.cross_entropy(out, labels)   \n",
    "        acc = accuracy(out, labels)           \n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   \n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      \n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "\n",
    "# Training and assessment functions\n",
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history\n",
    "\n",
    "# Create a function to port stuff to GPU\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_default_device()\n",
    "\n",
    "# create a function for transferring data to the GPU\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "# functions for displaying the error and accuracy of the model\n",
    "def plot_losses(history):\n",
    "    losses = [x['val_loss'] for x in history]\n",
    "    plt.plot(losses, '-x')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Loss vs. No. of epochs')\n",
    "\n",
    "def plot_accuracies(history):\n",
    "    accuracies = [x['val_acc'] for x in history]\n",
    "    plt.plot(accuracies, '-x')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy vs. No. of epochs')\n",
    "    \n",
    "# FGSM attack\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return sign_data_grad, perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(target_model, defense_model, device, test_loader, epsilon):\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Remember gradients\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        sign_data_grad, perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        target_output = target_model(perturbed_data)\n",
    "        \n",
    "        # Check for success\n",
    "        target_pred = target_output.max(1, keepdim=True)[1]\n",
    "        \n",
    "        print(target_pred)\n",
    "        \n",
    "        break\n",
    "        \n",
    "    return data, target, perturbed_data, sign_data_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = 'new-model.pth'\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_ds, batch_size=1, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target, perturbed_image, sign_data_grad = test(model, model, device, test_loader, 0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(data[0].data.detach().cpu().numpy().transpose(1,2,0))\n",
    "plt.savefig(\"figures/original-kiwi.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(sign_data_grad[0].data.detach().cpu().numpy().transpose(1,2,0))\n",
    "plt.savefig(\"figures/noise.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(perturbed_image[0].data.detach().cpu().numpy().transpose(1,2,0))\n",
    "plt.savefig(\"figures/attacked.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(sign_data_grad)\n",
    "output.max(1, keepdim=True)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Port data to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-28T14:07:53.879674Z",
     "iopub.status.busy": "2020-08-28T14:07:53.878011Z",
     "iopub.status.idle": "2020-08-28T14:07:53.880684Z",
     "shell.execute_reply": "2020-08-28T14:07:53.881216Z"
    },
    "papermill": {
     "duration": 0.02807,
     "end_time": "2020-08-28T14:07:53.88133",
     "exception": false,
     "start_time": "2020-08-28T14:07:53.85326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uploading data to the GPU\n",
    "train_loader = DeviceDataLoader(train_loader, device)\n",
    "val_loader = DeviceDataLoader(val_loader, device)\n",
    "test_loader = DeviceDataLoader(test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convo model (258 x 258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extending the original model by adding neural network architecture\n",
    "class FruitRecognitionModel(ImageClassificationBase):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(1032256, 128)\n",
    "        self.fc2 = nn.Linear(128, out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        x = self.conv1(x)\n",
    "        # Use the rectified-linear activation function over x\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Run max pooling over x\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # Pass data through dropout1\n",
    "        x = self.dropout1(x)\n",
    "        # Flatten x with start_dim=1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Pass data through fc1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Apply softmax to x\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convo model (128 x 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extending the original model by adding neural network architecture\n",
    "class FruitRecognitionModel(ImageClassificationBase):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(246016, 128)\n",
    "        self.fc2 = nn.Linear(128, out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        x = self.conv1(x)\n",
    "        # Use the rectified-linear activation function over x\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Run max pooling over x\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # Pass data through dropout1\n",
    "        x = self.dropout1(x)\n",
    "        # Flatten x with start_dim=1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Pass data through fc1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Apply softmax to x\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convo model (64 x 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extending the original model by adding neural network architecture\n",
    "class FruitRecognitionModel(ImageClassificationBase):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(57600, 128)\n",
    "        self.fc2 = nn.Linear(128, out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        x = self.conv1(x)\n",
    "        # Use the rectified-linear activation function over x\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Run max pooling over x\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # Pass data through dropout1\n",
    "        x = self.dropout1(x)\n",
    "        # Flatten x with start_dim=1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Pass data through fc1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Apply softmax to x\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Five-layer linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extending the original model by adding neural network architecture\n",
    "class FruitRecognitionModel(ImageClassificationBase):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_size, 1024)\n",
    "        self.linear2 = nn.Linear(1024, 512)\n",
    "        self.linear3 = nn.Linear(512, 128)\n",
    "        self.linear4 = nn.Linear(128, 32)\n",
    "        self.linear5 = nn.Linear(32, out_size)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        # Flatten images into vectors\n",
    "        out = xb.view(xb.size(0), -1)\n",
    "        # Apply layers & activation functions\n",
    "        out = self.linear1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear4(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear5(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convo model (padded 264 x 264)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extending the original model by adding neural network architecture\n",
    "class FruitRecognitionModel(ImageClassificationBase):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(1081600, 128)\n",
    "        self.fc2 = nn.Linear(128, out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        x = self.conv1(x)\n",
    "        # Use the rectified-linear activation function over x\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Run max pooling over x\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # Pass data through dropout1\n",
    "        x = self.dropout1(x)\n",
    "        # Flatten x with start_dim=1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Pass data through fc1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Apply softmax to x\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = 258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the size of the input (image tensor size) and\n",
    "# output (number of classes)\n",
    "input_size = 3*pixels*pixels\n",
    "output_size = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-28T14:07:54.039886Z",
     "iopub.status.busy": "2020-08-28T14:07:54.038989Z",
     "iopub.status.idle": "2020-08-28T14:07:54.394827Z",
     "shell.execute_reply": "2020-08-28T14:07:54.396236Z"
    },
    "papermill": {
     "duration": 0.396674,
     "end_time": "2020-08-28T14:07:54.396428",
     "exception": false,
     "start_time": "2020-08-28T14:07:53.999754",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# porting the model to the GPU\n",
    "model = to_device(FruitRecognitionModel(input_size, output_size), device)\n",
    "to_device(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-28T14:07:54.479646Z",
     "iopub.status.busy": "2020-08-28T14:07:54.475237Z",
     "iopub.status.idle": "2020-08-28T14:08:04.486409Z",
     "shell.execute_reply": "2020-08-28T14:08:04.485881Z"
    },
    "papermill": {
     "duration": 10.05415,
     "end_time": "2020-08-28T14:08:04.486519",
     "exception": false,
     "start_time": "2020-08-28T14:07:54.432369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# check the accuracy and losses on the validation sample\n",
    "history = [evaluate(model, val_loader)]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# we train the model, choosing empirically the optimal step and the number of epochs\n",
    "history += fit(10, 0.01, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a graph of losses\n",
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a graph of accuracy\n",
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the model on a test sample\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), '258-pixels-8-bits-convo.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack padded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:\\\\Users\\\\nguye\\\\OneDrive\\\\Documents\\\\0.05 new model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the original images to one size and transform them into tensors\n",
    "dataset = ImageFolder(data_dir, composed)\n",
    "print(\"There are {} images.\".format(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 10000\n",
    "test_size = 7000\n",
    "train_size = len(dataset) - val_size - test_size\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the batch size and create loaders\n",
    "batch_size = 20\n",
    "train_loader = DataLoader(train_ds, batch_size, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size*2, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size*2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading data to the GPU\n",
    "train_loader = DeviceDataLoader(train_loader, device)\n",
    "val_loader = DeviceDataLoader(val_loader, device)\n",
    "test_loader = DeviceDataLoader(test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack convo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:\\\\Users\\\\nguye\\\\OneDrive\\\\Documents\\\\0.05 new model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = 258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the original images to one size and transform them into tensors\n",
    "dsize = (pixels, pixels)\n",
    "composed = Compose([Resize(dsize), ToTensor()])\n",
    "\n",
    "dataset = ImageFolder(data_dir, composed)\n",
    "print(\"There are {} images.\".format(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 10000\n",
    "test_size = 7000\n",
    "train_size = len(dataset) - val_size - test_size\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the batch size and create loaders\n",
    "batch_size = 20\n",
    "train_loader = DataLoader(train_ds, batch_size, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size*2, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size*2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading data to the GPU\n",
    "train_loader = DeviceDataLoader(train_loader, device)\n",
    "val_loader = DeviceDataLoader(val_loader, device)\n",
    "test_loader = DeviceDataLoader(test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3*258*258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extending the original model by adding neural network architecture\n",
    "class FruitRecognitionModel(ImageClassificationBase):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(1032256, 128)\n",
    "        self.fc2 = nn.Linear(128, out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        x = self.conv1(x)\n",
    "        # Use the rectified-linear activation function over x\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Run max pooling over x\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # Pass data through dropout1\n",
    "        x = self.dropout1(x)\n",
    "        # Flatten x with start_dim=1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Pass data through fc1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Apply softmax to x\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# porting the model to the GPU\n",
    "model = to_device(FruitRecognitionModel(input_size, out_size=output_size), device)\n",
    "to_device(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = 'new-model.pth'\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack normal & padded models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the resulting classes from directories\n",
    "data_dir = 'C:\\\\Users\\\\nguye\\\\OneDrive\\\\Documents\\\\Fruits'\n",
    "classes = os.listdir(data_dir)\n",
    "print(\"There are {} fruits.\".format(len(classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the original images to one size and transform them into tensors\n",
    "pixels = 258\n",
    "dsize = (pixels, pixels)\n",
    "composed = Compose([Resize(dsize), ToTensor()])\n",
    "\n",
    "dataset = ImageFolder(data_dir, composed)\n",
    "print(\"There are {} images.\".format(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training, validation and test sets\n",
    "val_size = 10500\n",
    "test_size = 7000\n",
    "train_size = len(dataset) - val_size - test_size\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(target_model, defense_model, device, test_loader, epsilon):\n",
    "    # Accuracy counter\n",
    "    target_correct = 0\n",
    "    defense_correct = 0\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Remember gradients\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        target_output = target_model(perturbed_data)\n",
    "        \n",
    "        # Defense model\n",
    "        perturbed_data = ToPILImage()(perturbed_data[0]).convert(\"RGB\")\n",
    "        perturbed_data = composed(perturbed_data).unsqueeze(0).to(device='cuda')\n",
    "        defense_output = defense_model(perturbed_data)\n",
    "\n",
    "        # Check for success\n",
    "        target_pred = target_output.max(1, keepdim=True)[1]\n",
    "        defense_pred = defense_output.max(1, keepdim=True)[1]\n",
    "        \n",
    "        if target_pred.item() == target.item():\n",
    "            target_correct += 1\n",
    "        if defense_pred.item() == target.item():\n",
    "            defense_correct += 1\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    target_acc = target_correct/float(len(test_loader))\n",
    "    defense_acc = defense_correct/float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTarget Accuracy = {} / {} = {}\".format(epsilon, target_correct, len(test_loader), target_acc))\n",
    "    print(\"Epsilon: {}\\tDefense Accuracy = {} / {} = {}\".format(epsilon, defense_correct, len(test_loader), defense_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [.01, .02, .05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# porting the model to the GPU\n",
    "model = to_device(FruitRecognitionModel(input_size, output_size), device)\n",
    "to_device(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained_model = 'new-model.pth'\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# porting the model to the GPU\n",
    "model1 = to_device(FruitRecognitionModel(input_size, output_size), device)\n",
    "to_device(model1, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = '264-pixels-resized-padded-convo.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = '258-pixels-8-bits-convo.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_ds, batch_size=1, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    test(model, model, device, test_loader, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test(model, model1, device, test_loader, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack normal & bit-depth model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce bit-depth\n",
    "bits = 8\n",
    "\n",
    "class BitDepthReduction:\n",
    "    def __call__(self, image):\n",
    "        image = TF.pil_to_tensor(image).float() / 255\n",
    "        return torch.round(image * (2^bits - 1))/(2^bits - 1) * 255\n",
    "\n",
    "composed = Compose([Resize((pixels, pixels)), BitDepthReduction()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(target_model, defense_model, device, test_loader, epsilon):\n",
    "    # Accuracy counter\n",
    "    target_correct = 0\n",
    "    defense_correct = 0\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Remember gradients\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        target_output = target_model(perturbed_data)\n",
    "        \n",
    "        # Defense model\n",
    "        perturbed_data = ToPILImage()(perturbed_data[0]).convert(\"RGB\")\n",
    "        perturbed_data = composed(perturbed_data).unsqueeze(0).to(device='cuda')\n",
    "        defense_output = defense_model(perturbed_data)\n",
    "\n",
    "        # Check for success\n",
    "        target_pred = target_output.max(1, keepdim=True)[1]\n",
    "        defense_pred = defense_output.max(1, keepdim=True)[1]\n",
    "        \n",
    "        if target_pred.item() == target.item():\n",
    "            target_correct += 1\n",
    "        if defense_pred.item() == target.item():\n",
    "            defense_correct += 1\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    target_acc = target_correct/float(len(test_loader))\n",
    "    defense_acc = defense_correct/float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTarget Accuracy = {} / {} = {}\".format(epsilon, target_correct, len(test_loader), target_acc))\n",
    "    print(\"Epsilon: {}\\tDefense Accuracy = {} / {} = {}\".format(epsilon, defense_correct, len(test_loader), defense_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [.03, .04, .06]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_ds, batch_size=1, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = 'new-model.pth'\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = '258-pixels-8-bits-convo.pth'\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 8 bits with 255 RGB scale\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    test(model, model, device, test_loader, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 20 bits\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    test(model, model, device, test_loader, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 10 bits\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    test(model, model, device, test_loader, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 6 bits\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    test(model, model, device, test_loader, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 5 bits\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    test(model, model, device, test_loader, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 1 bit\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    test(model, model, device, test_loader, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means clustering with bit-depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_clustering(data, no_of_clusters, centroids=None, maximum_counter=300, tolerance=1e-9, \\\n",
    "                       print_output=10):    \n",
    "    # Initialise centroids if blank:\n",
    "    if centroids is None:\n",
    "        centroids = np.random.normal(0, 1, size=(no_of_clusters, data.shape[-1]))\n",
    "    else:\n",
    "        centroids = centroids.astype(np.float)\n",
    "    \n",
    "    for N in range(maximum_counter):\n",
    "        if N > 0:\n",
    "            L_old = L_new\n",
    "        # The assignments:\n",
    "        assignments = np.zeros((data.shape[0],no_of_clusters))\n",
    "        \n",
    "        # Distance to centroids\n",
    "        dist = cdist(data, centroids)\n",
    "        \n",
    "        # Assignments:\n",
    "        boolean = np.argmin(dist, axis = 1)\n",
    "        for i in range(len(boolean)):\n",
    "            assignments[i, boolean[i]] = 1\n",
    "        \n",
    "        # Objective function:\n",
    "        L_new = np.sum(assignments*dist**2)\n",
    "        \n",
    "        # Recalculate centroids:\n",
    "        for i in np.unique(boolean):\n",
    "            centroids[i,:] = np.mean(data[[j == i for j in boolean]], axis = 0)\n",
    "        \n",
    "        if N > 0:\n",
    "            if (L_old - L_new)/L_new <= tolerance:\n",
    "                return centroids, assignments\n",
    "        \n",
    "    return centroids, assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "no_of_bits = 8\n",
    "centroids, assignments = k_means_clustering(images[0].data.detach().cpu().numpy().transpose(1,2,0).reshape(258*258, 3), 2**no_of_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximation = np.matmul(assignments, centroids).reshape(258,258,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(15, 5))\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(images[0].data.detach().cpu().numpy().transpose(1,2,0))\n",
    "plt.axis('off')\n",
    "plt.tight_layout;\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(approximation)\n",
    "plt.axis('off')\n",
    "plt.tight_layout;\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(target_model, defense_model, device, test_loader, epsilon):\n",
    "    # Accuracy counter\n",
    "    target_correct = 0\n",
    "    defense_correct = 0\n",
    "    counter = 0\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:        \n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Remember gradients\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        target_output = target_model(perturbed_data)\n",
    "        \n",
    "        # Defense model\n",
    "        centroids, assignments = k_means_clustering(perturbed_data[0].data.detach().cpu().numpy().transpose(1,2,0).reshape(258*258,3), 2**no_of_bits)\n",
    "        perturbed_data = torch.FloatTensor(np.matmul(assignments, centroids).reshape(1,3,258,258)).to(device='cuda')\n",
    "        defense_output = defense_model(perturbed_data)\n",
    "\n",
    "        # Check for success\n",
    "        target_pred = target_output.max(1, keepdim=True)[1]\n",
    "        defense_pred = defense_output.max(1, keepdim=True)[1]\n",
    "        \n",
    "        if target_pred.item() == target.item():\n",
    "            target_correct += 1\n",
    "        if defense_pred.item() == target.item():\n",
    "            defense_correct += 1\n",
    "            \n",
    "        counter += 1\n",
    "        print(counter)\n",
    "        if counter == 100:\n",
    "            break\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    target_acc = target_correct/float(len(test_loader))\n",
    "    defense_acc = defense_correct/float(len(test_loader))\n",
    "    print(epsilon)\n",
    "    print(target_correct)\n",
    "    print(defense_correct)\n",
    "    print(\"Epsilon: {}\\tTarget Accuracy = {} / {} = {}\".format(epsilon, target_correct, len(test_loader), target_acc))\n",
    "    print(\"Epsilon: {}\\tDefense Accuracy = {} / {} = {}\".format(epsilon, defense_correct, len(test_loader), defense_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_ds, batch_size=1, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 8 bits with 255 RGB scale\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    test(model, model, device, test_loader, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single pattern attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New model (258 x 258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# we train the model, choosing empirically the optimal step and the number of epochs\n",
    "history += fit(15, 0.01, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a graph of losses\n",
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a graph of accuracy\n",
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the model on a test sample\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'new-model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 layers model (150 x 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# we train the model, choosing empirically the optimal step and the number of epochs\n",
    "history += fit(13, 0.01, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a graph of losses\n",
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a graph of accuracy\n",
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), '5-layer-150.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 layers model (258 x 258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-28T14:08:04.533455Z",
     "iopub.status.busy": "2020-08-28T14:08:04.532438Z",
     "iopub.status.idle": "2020-08-28T14:37:43.781616Z",
     "shell.execute_reply": "2020-08-28T14:37:43.782498Z"
    },
    "papermill": {
     "duration": 1779.277114,
     "end_time": "2020-08-28T14:37:43.782717",
     "exception": false,
     "start_time": "2020-08-28T14:08:04.505603",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# we train the model, choosing empirically the optimal step and the number of epochs\n",
    "history += fit(13, 0.01, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-28T14:37:43.828101Z",
     "iopub.status.busy": "2020-08-28T14:37:43.826905Z",
     "iopub.status.idle": "2020-08-28T14:37:43.991843Z",
     "shell.execute_reply": "2020-08-28T14:37:43.992368Z"
    },
    "papermill": {
     "duration": 0.189767,
     "end_time": "2020-08-28T14:37:43.992514",
     "exception": false,
     "start_time": "2020-08-28T14:37:43.802747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build a graph of losses\n",
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-28T14:37:44.045618Z",
     "iopub.status.busy": "2020-08-28T14:37:44.043981Z",
     "iopub.status.idle": "2020-08-28T14:37:44.196544Z",
     "shell.execute_reply": "2020-08-28T14:37:44.197101Z"
    },
    "papermill": {
     "duration": 0.182878,
     "end_time": "2020-08-28T14:37:44.197261",
     "exception": false,
     "start_time": "2020-08-28T14:37:44.014383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build a graph of accuracy\n",
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-28T14:37:44.245108Z",
     "iopub.status.busy": "2020-08-28T14:37:44.243421Z",
     "iopub.status.idle": "2020-08-28T14:38:02.233507Z",
     "shell.execute_reply": "2020-08-28T14:38:02.232116Z"
    },
    "papermill": {
     "duration": 18.014859,
     "end_time": "2020-08-28T14:38:02.233623",
     "exception": false,
     "start_time": "2020-08-28T14:37:44.218764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check the model on a test sample\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-28T14:38:02.285429Z",
     "iopub.status.busy": "2020-08-28T14:38:02.279535Z",
     "iopub.status.idle": "2020-08-28T14:38:02.436736Z",
     "shell.execute_reply": "2020-08-28T14:38:02.43613Z"
    },
    "papermill": {
     "duration": 0.182271,
     "end_time": "2020-08-28T14:38:02.436855",
     "exception": false,
     "start_time": "2020-08-28T14:38:02.254584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), '5-layer-model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = 'new-model.pth'\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbation = Variable(torch.zeros(3, pixels, pixels, requires_grad=True).to(device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepsize = 0.01\n",
    "optimiser = optim.SGD(model.parameters(), lr=stepsize, momentum=0.7)\n",
    "epsilon = 30\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for data, target in train_loader:\n",
    "    data.requires_grad = True\n",
    "    perturbation.requires_grad = True\n",
    "    perturbation.retain_grad()\n",
    "    output_1 = model(data)\n",
    "    output_2 = model(data + perturbation.repeat(10,1,1,1))\n",
    "\n",
    "    loss_1 = F.nll_loss(output_1, target)\n",
    "    loss_2 = F.nll_loss(output_2, target)\n",
    "    loss = (loss_1 + loss_2) / 2\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimiser.step()\n",
    "     \n",
    "    perturbation.data = perturbation.data + stepsize * perturbation.grad\n",
    "    perturbation.data = (perturbation.data / torch.norm(perturbation.data.view(-1, 66564))\n",
    "                        * epsilon)\n",
    "    perturbation.data[perturbation.data != perturbation.data] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(perturbation.data.detach().cpu().numpy().transpose(1, 2, 0))\n",
    "plt.savefig(\"figures/universal-pertubation.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), '258-pixels-convo-ad-trained2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), '258-pixels-convo-ad-trained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "# Show a normal image\n",
    "plt.axis('off')\n",
    "plt.imshow(images[0].data.detach().cpu().numpy().transpose(1,2,0))\n",
    "plt.savefig(\"figures/before.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an altered image\n",
    "plt.axis('off')\n",
    "plt.imshow((images[0]+perturbation.data).data.detach().cpu().numpy().transpose(1,2,0))\n",
    "plt.savefig(\"figures/after.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DeviceDataLoader(test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, test_loader, perturbation):\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "    \n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "        # Forward pass the data through the model\n",
    "        perturbed_data = data + perturbation\n",
    "        output = model(perturbed_data)\n",
    "        \n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1][0]\n",
    "        \n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "        \n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(\"Test Accuracy = {} / {} = {}\".format(correct, len(test_loader), final_acc))\n",
    "\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(3, 258, 258).to(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_1, ex_1 = test_accuracy(model, test_loader, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_2, ex_2 = test_accuracy(model, test_loader, perturbation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = 'new-model.pth'\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_3, ex_3 = test_accuracy(model, test_loader, perturbation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save attacked images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM attack code\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = 'new-model.pth'\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader_1 = torch.utils.data.DataLoader(dataset, batch_size=1, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genadversarial(model, device, test_loader):\n",
    "    i = 0\n",
    "    for data, target in test_loader:\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "    \n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, 0.05, data_grad)\n",
    "        plt.imsave('0.05 new model\\\\{}\\\\{}.png'.format(classes[int(target)],i), perturbed_data.detach().cpu().numpy()[0].transpose(1,2,0))\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "genadversarial(model, device, trainloader_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:\\\\Users\\\\nguye\\\\OneDrive\\\\Documents\\\\Fruits (2)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the original images to one size and transform them into tensors\n",
    "dataset = ImageFolder(data_dir, composed)\n",
    "print(\"There are {} images.\".format(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and validation samples from the initial dataset\n",
    "torch.manual_seed(43)\n",
    "\n",
    "val_size = 20000\n",
    "test_size = 14000\n",
    "train_size = len(dataset) - val_size - test_size\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the batch size and create loaders for the resulting samples using DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size*2, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# porting the model to the GPU\n",
    "model = to_device(FruitRecognitionModel(input_size, out_size=output_size), device)\n",
    "to_device(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading data to the GPU\n",
    "train_loader = DeviceDataLoader(train_loader, device)\n",
    "val_loader = DeviceDataLoader(val_loader, device)\n",
    "test_loader = DeviceDataLoader(test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the accuracy and losses on the validation sample\n",
    "history = [evaluate(model, val_loader)]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# we train the model, choosing empirically the optimal step and the number of epochs\n",
    "history += fit(10, 0.01, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a graph of losses\n",
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build a graph of accuracy\n",
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the model on a test sample\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), '5-layer-model-robust.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack robust model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:\\\\Users\\\\nguye\\\\OneDrive\\\\Documents\\\\Fruits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(data_dir, composed)\n",
    "print(\"There are {} images.\".format(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 10500\n",
    "test_size = 7000\n",
    "train_size = len(dataset) - val_size - test_size\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DeviceDataLoader(test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0, .01, .02, .03, .04, .05, .06]\n",
    "pretrained_model = '5-layer-model-robust.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_ds, batch_size=1, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test( model, device, test_loader, epsilon ):\n",
    "\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_data)\n",
    "\n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if (epsilon == 0) and (len(adv_examples) < 5):\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "        else:\n",
    "            # Save some adv examples for visualization later\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = test(model, device, test_loader, eps)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, accuracies, \"*-\")\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, .065, step=0.01))\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0, .01, .02, .03, .04, .05, .06]\n",
    "pretrained_model = \"fruit-recognition-model.pth\"\n",
    "use_cuda=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.utils import _pair, _quadruple\n",
    "\n",
    "\n",
    "class MedianPool2d(nn.Module):\n",
    "    \"\"\" Median pool (usable as median filter when stride=1) module.\n",
    "    \n",
    "    Args:\n",
    "         kernel_size: size of pooling kernel, int or 2-tuple\n",
    "         stride: pool stride, int or 2-tuple\n",
    "         padding: pool padding, int or 4-tuple (l, r, t, b) as in pytorch F.pad\n",
    "         same: override padding and enforce same padding, boolean\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size=3, stride=1, padding=0, same=False):\n",
    "        super(MedianPool2d, self).__init__()\n",
    "        self.k = _pair(kernel_size)\n",
    "        self.stride = _pair(stride)\n",
    "        self.padding = _quadruple(padding)  # convert to l, r, t, b\n",
    "        self.same = same\n",
    "\n",
    "    def _padding(self, x):\n",
    "        if self.same:\n",
    "            ih, iw = x.size()[2:]\n",
    "            if ih % self.stride[0] == 0:\n",
    "                ph = max(self.k[0] - self.stride[0], 0)\n",
    "            else:\n",
    "                ph = max(self.k[0] - (ih % self.stride[0]), 0)\n",
    "            if iw % self.stride[1] == 0:\n",
    "                pw = max(self.k[1] - self.stride[1], 0)\n",
    "            else:\n",
    "                pw = max(self.k[1] - (iw % self.stride[1]), 0)\n",
    "            pl = pw // 2\n",
    "            pr = pw - pl\n",
    "            pt = ph // 2\n",
    "            pb = ph - pt\n",
    "            padding = (pl, pr, pt, pb)\n",
    "        else:\n",
    "            padding = self.padding\n",
    "        return padding\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # using existing pytorch functions and tensor ops so that we get autograd, \n",
    "        # would likely be more efficient to implement from scratch at C/Cuda level\n",
    "        x = F.pad(x, self._padding(x), mode='reflect')\n",
    "        x = x.unfold(2, self.k[0], self.stride[0]).unfold(3, self.k[1], self.stride[1])\n",
    "        x = x.contiguous().view(x.size()[:4] + (-1,)).median(dim=-1)[0]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MedianPool2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test( model, device, test_loader, epsilon ):\n",
    "\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        perturbed_data = tfms(perturbed_data)\n",
    "        \n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_data)\n",
    "        \n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if (epsilon == 0) and (len(adv_examples) < 5):\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "        else:\n",
    "            # Save some adv examples for visualization later\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "tfms = Compose([torchvision.transforms.Resize(250), torchvision.transforms.Pad(4)])\n",
    "\n",
    "dataloader_iterator = iter(test_loader)\n",
    "for i in range(1):\n",
    "    try:\n",
    "        data, target = next(dataloader_iterator)\n",
    "    except StopIteration:\n",
    "        dataloader_iterator = iter(dataloader)\n",
    "        data, target = next(dataloader_iterator)\n",
    "    data = tfms(data)\n",
    "    print(plt.imshow(data[0].detach().cpu().numpy().transpose(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = test(model, device, test_loader, eps)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, accuracies, \"*-\")\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, .065, step=0.01))\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal FGSM attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0, .01, .02, .03, .04, .05, .06]\n",
    "pretrained_model = \"264-pixels-resized-padded-convo.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, epsilon):\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "        print(\"done\")\n",
    "        \n",
    "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_data)\n",
    "\n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if (epsilon == 0) and (len(adv_examples) < 5):\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "        else:\n",
    "            # Save some adv examples for visualization later\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_ds, batch_size=1, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = test(model, device, test_loader, eps)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, accuracies, \"*-\")\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, .065, step=0.01))\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot several examples of adversarial samples at each epsilon\n",
    "cnt = 0\n",
    "plt.figure(figsize=(11,10))\n",
    "for i in range(len(epsilons)):\n",
    "    for j in range(len(examples[i])):\n",
    "        cnt += 1\n",
    "        plt.subplot(len(epsilons),len(examples[0]),cnt)\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        if j == 0:\n",
    "            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n",
    "        orig,adv,ex = examples[i][j]\n",
    "        orig = classes[orig]\n",
    "        adv = classes[adv]\n",
    "        plt.title(\"{} -> {}\".format(orig, adv))\n",
    "        plt.imshow(ex.transpose(1, 2, 0))\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/attack1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM attack code\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0, .01, .02, .03, .04, .05, .06]\n",
    "pretrained_model = \"new-model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test( model, device, test_loader, epsilon ):\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_data)\n",
    "\n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if (epsilon == 0) and (len(adv_examples) < 5):\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "        else:\n",
    "            # Save some adv examples for visualization later\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_ds, batch_size=1, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = test(model, device, test_loader, eps)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, accuracies, \"*-\")\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, .065, step=0.01))\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = Compose([torchvision.transforms.Resize(256), torchvision.transforms.Pad(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test( model, device, test_loader, epsilon ):\n",
    "\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        perturbed_data = tfms(perturbed_data)\n",
    "        \n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_data)\n",
    "        \n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if (epsilon == 0) and (len(adv_examples) < 5):\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "        else:\n",
    "            # Save some adv examples for visualization later\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = test(model, device, test_loader, eps)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, accuracies, \"*-\")\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, .065, step=0.01))\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementation of sample defense.\n",
    "This defense loads inception resnet v2 checkpoint and classifies all images\n",
    "using loaded checkpoint.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from scipy.misc import imread\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import inception_resnet_v2\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    'master', '', 'The address of the TensorFlow master to use.')\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    'checkpoint_path', '', 'Path to checkpoint for inception network.')\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    'input_dir', '', 'Input directory with images.')\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    'output_file', '', 'Output file to save labels.')\n",
    "\n",
    "tf.flags.DEFINE_integer(\n",
    "    'image_width', 299, 'Width of each input images.')\n",
    "\n",
    "tf.flags.DEFINE_integer(\n",
    "    'image_height', 299, 'Height of each input images.')\n",
    "\n",
    "tf.flags.DEFINE_integer(\n",
    "    'batch_size', 16, 'How many images process at one time.')\n",
    "\n",
    "tf.flags.DEFINE_integer(\n",
    "    'image_resize', 331, 'Resize of image size.')\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "\n",
    "\n",
    "def padding_layer_iyswim(inputs, shape, name=None):\n",
    "    h_start = shape[0]\n",
    "    w_start = shape[1]\n",
    "    output_short = shape[2]\n",
    "    input_shape = tf.shape(inputs)\n",
    "    input_short = tf.reduce_min(input_shape[1:3])\n",
    "    input_long = tf.reduce_max(input_shape[1:3])\n",
    "    output_long = tf.to_int32(tf.ceil(\n",
    "        1. * tf.to_float(output_short) * tf.to_float(input_long) / tf.to_float(input_short)))\n",
    "    output_height = tf.to_int32(input_shape[1] >= input_shape[2]) * output_long +\\\n",
    "        tf.to_int32(input_shape[1] < input_shape[2]) * output_short\n",
    "    output_width = tf.to_int32(input_shape[1] >= input_shape[2]) * output_short +\\\n",
    "        tf.to_int32(input_shape[1] < input_shape[2]) * output_long\n",
    "    return tf.pad(inputs, tf.to_int32(tf.stack([[0, 0], [h_start, output_height - h_start - input_shape[1]], [w_start, output_width - w_start - input_shape[2]], [0, 0]])), name=name)\n",
    "\n",
    "\n",
    "def load_images(input_dir, batch_shape):\n",
    "    \"\"\"Read png images from input directory in batches.\n",
    "    Args:\n",
    "      input_dir: input directory\n",
    "      batch_shape: shape of minibatch array, i.e. [batch_size, height, width, 3]\n",
    "    Yields:\n",
    "      filenames: list file names without path of each image\n",
    "        Lenght of this list could be less than batch_size, in this case only\n",
    "        first few images of the result are elements of the minibatch.\n",
    "      images: array with all images from this batch\n",
    "    \"\"\"\n",
    "    images = np.zeros(batch_shape)\n",
    "    filenames = []\n",
    "    idx = 0\n",
    "    batch_size = batch_shape[0]\n",
    "    for filepath in tf.gfile.Glob(os.path.join(input_dir, '*.png')):\n",
    "        with tf.gfile.Open(filepath) as f:\n",
    "            image = imread(f, mode='RGB').astype(np.float) / 255.0\n",
    "        # Images for inception classifier are normalized to be in [-1, 1] interval.\n",
    "        images[idx, :, :, :] = image * 2.0 - 1.0\n",
    "        filenames.append(os.path.basename(filepath))\n",
    "        idx += 1\n",
    "        if idx == batch_size:\n",
    "            yield filenames, images\n",
    "            filenames = []\n",
    "            images = np.zeros(batch_shape)\n",
    "            idx = 0\n",
    "    if idx > 0:\n",
    "        yield filenames, images\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    batch_shape = [FLAGS.batch_size, FLAGS.image_height, FLAGS.image_width, 3]\n",
    "    num_classes = 1001\n",
    "    itr = 30\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        # Prepare graph\n",
    "        x_input = tf.placeholder(tf.float32, shape=batch_shape)\n",
    "        img_resize_tensor = tf.placeholder(tf.int32, [2])\n",
    "        x_input_resize = tf.image.resize_images(x_input, img_resize_tensor, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "        shape_tensor = tf.placeholder(tf.int32, [3])\n",
    "        padded_input = padding_layer_iyswim(x_input_resize, shape_tensor)\n",
    "        # 330 is the last value to keep 8*8 output, 362 is the last value to keep 9*9 output, stride = 32\n",
    "        padded_input.set_shape(\n",
    "            (FLAGS.batch_size, FLAGS.image_resize, FLAGS.image_resize, 3))\n",
    "\n",
    "        with slim.arg_scope(inception_resnet_v2.inception_resnet_v2_arg_scope()):\n",
    "            _, end_points = inception_resnet_v2.inception_resnet_v2(\n",
    "                padded_input, num_classes=num_classes, is_training=False, create_aux_logits=True)\n",
    "\n",
    "        predicted_labels = tf.argmax(end_points['Predictions'], 1)\n",
    "\n",
    "        # Run computation\n",
    "        saver = tf.train.Saver(slim.get_model_variables())\n",
    "        session_creator = tf.train.ChiefSessionCreator(\n",
    "            scaffold=tf.train.Scaffold(saver=saver),\n",
    "            checkpoint_filename_with_path=FLAGS.checkpoint_path,\n",
    "            master=FLAGS.master)\n",
    "\n",
    "        with tf.train.MonitoredSession(session_creator=session_creator) as sess:\n",
    "            with tf.gfile.Open(FLAGS.output_file, 'w') as out_file:\n",
    "                for filenames, images in load_images(FLAGS.input_dir, batch_shape):\n",
    "                    final_preds = np.zeros(\n",
    "                        [FLAGS.batch_size, num_classes, itr])\n",
    "                    for j in range(itr):\n",
    "                        if np.random.randint(0, 2, size=1) == 1:\n",
    "                            images = images[:, :, ::-1, :]\n",
    "                        resize_shape_ = np.random.randint(310, 331)\n",
    "                        pred, aux_pred = sess.run([end_points['Predictions'], end_points['AuxPredictions']],\n",
    "                                                        feed_dict={x_input: images, img_resize_tensor: [resize_shape_]*2,\n",
    "                                                                   shape_tensor: np.array([random.randint(0, FLAGS.image_resize - resize_shape_), random.randint(0, FLAGS.image_resize - resize_shape_), FLAGS.image_resize])})\n",
    "                        final_preds[..., j] = pred + 0.4 * aux_pred\n",
    "                    final_probs = np.sum(final_preds, axis=-1)\n",
    "                    labels = np.argmax(final_probs, 1)\n",
    "                    for filename, label in zip(filenames, labels):\n",
    "                        out_file.write('{0},{1}\\n'.format(filename, label))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 64x64 GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator\n",
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = '64-discriminative.pth'\n",
    "netD.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save generative model\n",
    "torch.save(netG.state_dict(), '64-generative.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save generative model\n",
    "torch.save(netD.state_dict(), '64-discriminative.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 128x128 GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 200\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 128\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Learning rate for optimizers; 0.0005 is too high\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 16, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 16),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*16) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 16 x 16 \n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 64 x 64\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 128 x 128\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the generator\n",
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 128 x 128\n",
    "            nn.Conv2d(nc, ndf, 4, stride=2, padding=1, bias=False), \n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 64 x 64\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 32 x 32\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 16 x 16 \n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 8 x 8\n",
    "            nn.Conv2d(ndf * 8, ndf * 16, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*16) x 4 x 4\n",
    "            nn.Conv2d(ndf * 16, 1, 4, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # state size. 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = '128-discriminative.pth'\n",
    "netD.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# From: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save generative model\n",
    "torch.save(netG.state_dict(), '128-generative.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save discriminative model\n",
    "torch.save(netD.state_dict(), '128-discriminative.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a picture close to original picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = '64-pixels-convo.pth'\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = '64-generative.pth'\n",
    "netG.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DeviceDataLoader(DataLoader(test_ds, batch_size=1, num_workers=4, pin_memory=True), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show a normal image\n",
    "plt.axis('off')\n",
    "plt.imshow(images[0].data.detach().cpu().numpy().transpose(1,2,0))\n",
    "plt.savefig('figures/original guava.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next image\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a normal image\n",
    "plt.axis('off')\n",
    "plt.imshow(images[0].data.detach().cpu().numpy().transpose(1,2,0))\n",
    "#plt.savefig('figures/original pear.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise GAN images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(image, model, loss_fn, R, L): \n",
    "    for i in range(R):\n",
    "        a = torch.randn(1, nz, 1, 1, device=device)\n",
    "        pred = model(a)\n",
    "        loss = loss_fn(pred, image)\n",
    "        if i == 0:\n",
    "            minimum_loss = loss\n",
    "            random = a\n",
    "        elif loss < minimum_loss:\n",
    "            minimum_loss = loss\n",
    "            random = a\n",
    "        \n",
    "    for i in range(L):\n",
    "        # Compute prediction and loss\n",
    "        random.requires_grad = True\n",
    "        random.retain_grad()\n",
    "        \n",
    "        pred = model(random)\n",
    "        loss = loss_fn(pred, image)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        random.data -= 0.1*random.grad.data\n",
    "    \n",
    "    return model(random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fig = plt.figure(figsize=(18.5, 10.5))\n",
    "columns = 7\n",
    "rows = 5\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "\n",
    "# ax enables access to manipulate each of subplots\n",
    "ax = []\n",
    "\n",
    "for i in range(rows):\n",
    "    # select image\n",
    "    data, target = next(dataiter)\n",
    "    img = data[0].data.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    # create subplot and append to ax\n",
    "    ax.append(fig.add_subplot(rows, columns, (i*columns)+1))\n",
    "    # set title\n",
    "    ax[0].set_title(\"Original\", fontdict = {'fontsize': 16}, fontproperties=font)\n",
    "    # remove axes\n",
    "    ax[-1].xaxis.set_visible(False)\n",
    "    ax[-1].yaxis.set_visible(False)\n",
    "    # put image to display\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # adversarial images\n",
    "    # create subplot and append to ax\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # Remember gradients\n",
    "    data.requires_grad = True\n",
    "    # Forward pass the data through the model\n",
    "    output = model(data)\n",
    "    # Calculate the loss\n",
    "    loss = F.nll_loss(output, target)\n",
    "    # Zero all existing gradients\n",
    "    model.zero_grad()\n",
    "    # Calculate gradients of model in backward pass\n",
    "    loss.backward()\n",
    "    # Collect datagrad\n",
    "    data_grad = data.grad.data\n",
    "    # Call FGSM Attack\n",
    "    perturbed_data = fgsm_attack(data, 0.05, data_grad)\n",
    "    display_perturbed_data = perturbed_data[0].data.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    ax.append(fig.add_subplot(rows, columns, (i*columns)+2))\n",
    "    # set title\n",
    "    ax[1].set_title(\"Adv\", fontdict = {'fontsize': 16}, fontproperties=font)\n",
    "    # remove axes\n",
    "    ax[-1].xaxis.set_visible(False)\n",
    "    ax[-1].yaxis.set_visible(False)\n",
    "    # put image to display\n",
    "    plt.imshow(display_perturbed_data)\n",
    "    \n",
    "    for j in range(1,6):\n",
    "        # adversarial images that change L\n",
    "        random = train_loop(perturbed_data, netG, loss_fn, 30000, 1000*j)\n",
    "        random = random[0].data.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "        ax.append(fig.add_subplot(rows, columns, (i*columns)+2+j))\n",
    "        # remove axes\n",
    "        ax[-1].xaxis.set_visible(False)\n",
    "        ax[-1].yaxis.set_visible(False)\n",
    "        # put image to display\n",
    "        plt.imshow(random)\n",
    "\n",
    "for i in range(2,7):\n",
    "    ax[i].set_title(\"L = {:,}\".format((i-1)*1000), fontdict = {'fontsize': 16}, fontproperties=font)\n",
    "\n",
    "# render plot\n",
    "plt.show()\n",
    "\n",
    "# save plot\n",
    "fig.savefig('figures/GAN-changes-2.png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig = plt.figure(figsize=(18.5, 10.5))\n",
    "columns = 7\n",
    "rows = 5\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "\n",
    "# ax enables access to manipulate each of subplots\n",
    "ax = []\n",
    "\n",
    "for i in range(rows):\n",
    "    # select image\n",
    "    data, target = next(dataiter)\n",
    "    img = data[0].data.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    # create subplot and append to ax\n",
    "    ax.append(fig.add_subplot(rows, columns, (i*columns)+1))\n",
    "    # set title\n",
    "    ax[0].set_title(\"Original\", fontdict = {'fontsize': 16}, fontproperties=font)\n",
    "    # remove axes\n",
    "    ax[-1].xaxis.set_visible(False)\n",
    "    ax[-1].yaxis.set_visible(False)\n",
    "    # put image to display\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # adversarial images\n",
    "    # create subplot and append to ax\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # Remember gradients\n",
    "    data.requires_grad = True\n",
    "    # Forward pass the data through the model\n",
    "    output = model(data)\n",
    "    # Calculate the loss\n",
    "    loss = F.nll_loss(output, target)\n",
    "    # Zero all existing gradients\n",
    "    model.zero_grad()\n",
    "    # Calculate gradients of model in backward pass\n",
    "    loss.backward()\n",
    "    # Collect datagrad\n",
    "    data_grad = data.grad.data\n",
    "    # Call FGSM Attack\n",
    "    perturbed_data = fgsm_attack(data, 0.05, data_grad)\n",
    "    display_perturbed_data = perturbed_data[0].data.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    ax.append(fig.add_subplot(rows, columns, (i*columns)+2))\n",
    "    # set title\n",
    "    ax[1].set_title(\"Adv\", fontdict = {'fontsize': 16}, fontproperties=font)\n",
    "    # remove axes\n",
    "    ax[-1].xaxis.set_visible(False)\n",
    "    ax[-1].yaxis.set_visible(False)\n",
    "    # put image to display\n",
    "    plt.imshow(display_perturbed_data)\n",
    "    \n",
    "    for j in range(1,6):\n",
    "        # adversarial images that change L\n",
    "        random = train_loop(perturbed_data, netG, loss_fn, 10000*j, 5000)\n",
    "        random = random[0].data.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "        ax.append(fig.add_subplot(rows, columns, (i*columns)+2+j))\n",
    "        # remove axes\n",
    "        ax[-1].xaxis.set_visible(False)\n",
    "        ax[-1].yaxis.set_visible(False)\n",
    "        # put image to display\n",
    "        plt.imshow(random)\n",
    "\n",
    "for i in range(2,7):\n",
    "    ax[i].set_title(\"R = {:,}\".format((i-1)*10000), fontdict = {'fontsize': 16}, fontproperties=font)\n",
    "\n",
    "# render plot\n",
    "plt.show()\n",
    "\n",
    "# save plot\n",
    "fig.savefig('figures/GAN-changes-R.png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "random = train_loop(images, netG, loss_fn, 30000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "random = train_loop(images, netG, loss_fn, 30000, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(random[0].permute(1,2,0).cpu().detach().numpy())\n",
    "#plt.savefig('figures/recreated pear.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See how GAN works with classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = '64-pixels-convo.pth'\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = 'new-model.pth'\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location=lambda storage, loc: storage.cuda(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = 0\n",
    "for i in range(100):\n",
    "    if torch.max(model(images), dim=1)[1].item() == 3:\n",
    "        success+=1\n",
    "print(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = 0\n",
    "for i in range(100):\n",
    "    if torch.max(model(random), dim=1)[1].item() == 3:\n",
    "        success+=1\n",
    "print(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM attack code\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(target_model, device, test_loader, epsilon):\n",
    "    # Accuracy counter\n",
    "    target_correct = 0\n",
    "    defense_correct = 0\n",
    "    i = 0\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Remember gradients\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        target_output = target_model(perturbed_data)\n",
    "        \n",
    "        # GAN-Defense model\n",
    "        perturbed_data = train_loop(perturbed_data.data, netG, loss_fn, 30000, 1000)\n",
    "        defense_output = target_model(perturbed_data)\n",
    "\n",
    "        # Check for success\n",
    "        target_pred = target_output.max(1, keepdim=True)[1]\n",
    "        defense_pred = defense_output.max(1, keepdim=True)[1]\n",
    "        \n",
    "        if target_pred.item() == target.item():\n",
    "            target_correct += 1\n",
    "        if defense_pred.item() == target.item():\n",
    "            defense_correct += 1\n",
    "    \n",
    "        i += 1\n",
    "        if i == 100:\n",
    "            break\n",
    "        \n",
    "    # Calculate final accuracy for this epsilon\n",
    "    target_acc = target_correct/float(len(test_loader))\n",
    "    defense_acc = defense_correct/float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTarget Accuracy = {} / {} = {}\".format(epsilon, target_correct, len(test_loader), target_acc))\n",
    "    print(\"Epsilon: {}\\tDefense Accuracy = {} / {} = {}\".format(epsilon, defense_correct, len(test_loader), defense_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    test(model, device, test_loader, eps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
